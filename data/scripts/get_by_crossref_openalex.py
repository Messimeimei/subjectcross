# -*- coding: utf-8 -*-
# Created by Messimeimei

"""
å¯å¢é‡è¡¥å……æ‰©å……æ•°æ®çš„è„šæœ¬ï¼šåŸºäº Crossref å’Œ OpenAlex æŠ“å–å…ƒæ•°æ®
    1. å¯¹äºå½“å‰ä¸‹è½½çš„æ•°æ®ï¼ˆdata/01meta_data/<å­¦ç§‘>ï¼‰ï¼Œ
       å¯¹æ¯”åŸå§‹æ•°æ®ï¼ˆdata/01origin_meta_data/<å­¦ç§‘>ï¼‰ï¼Œæ±‚å½“å‰æ•°æ®çš„æ–°å¢ DOI åˆ—è¡¨
    2. å¯¹æ–°å¢ DOI åˆ—è¡¨ï¼ŒæŠ“å– Crossref å’Œ OpenAlex å…ƒæ•°æ®
    3. ç›®æ ‡è·¯å¾„æ˜¯ data/02crossref_data/<å­¦ç§‘>.csvï¼Œå°†æ–°å¢ DOI çš„å®Œæ•´æ•°æ®è¿½åŠ è¿›å»ï¼Œå»é‡ï¼Œè¦†ç›–ä¿å­˜
"""

import random
import time
import os
import re
import json
import requests
import pandas as pd
from pathlib import Path
from tqdm import tqdm
from typing import List, Dict, Any


class CrossrefMetaProcessor:
    """
    è¿™ä¸ªç±»å®Œæˆæ•°æ®é›†æ„å»ºçš„ç¬¬ä¸€é˜¶æ®µï¼šä»01meta_dataç›®å½•è·å–åŸå§‹æ•°æ®ï¼Œ
    å¹¶é€šè¿‡Crossrefå’ŒOpenAlex APIæŠ“å–è¡¥å……å…ƒæ•°æ®ï¼Œæœ€åä¿å­˜åˆ°02crossref_dataå¯¹åº”å­¦ç§‘csvæ–‡ä»¶ä¸­
    """
    def __init__(self, input_dir: str,
                 output_dir: str = "data/02crossref_data",
                 openalex_email: str = "3170529323@qq.com",
                 origin_dir: str = None):     # ä¼ å…¥ 01origin_meta_data çš„å­¦ç§‘ç›®å½•ï¼Œç”¨äºå¢é‡è®¡ç®—

            ROOT_DIR = Path(__file__).resolve().parents[2]
            self.input_dir = str((ROOT_DIR / input_dir).resolve())          # 01meta_data/<å­¦ç§‘>
            self.output_dir = str((ROOT_DIR / output_dir).resolve())        # 02crossref_data
            self.map_path = ROOT_DIR / "data/deepseek_map.json"    # æ˜ å°„æ–‡ä»¶è·¯å¾„
            self.openalex_email = openalex_email
            self.df_raw = None
            self.df_merged = None
            self.origin_dir = str((ROOT_DIR / origin_dir).resolve()) if origin_dir else None


    # ================================ å·¥å…·å‡½æ•° ================================
    @staticmethod
    def _strip_tags(text: str) -> str:
        # å–å‡ºä»»æ„HTML/XMLæ ‡ç­¾ï¼Œå¹¶å°†è¿ç»­ç©ºç™½æ›¿æ¢ä¸ºå•ä¸ªç©ºæ ¼
        if not text:
            return ""
        txt = re.sub(r"<[^>]+>", "", text)
        txt = re.sub(r"\s+", " ", txt)
        return txt.strip()

    # ================================ Step 1. dfæ ¼å¼è¿”å›æ¸…æ´—åçš„ CSV ================================
    def load_all_csvs(self) -> pd.DataFrame:
        """
        åŠ è½½å­¦ç§‘ç›®å½•ä¸‹æ‰€æœ‰ CSV æ–‡ä»¶ï¼ˆè¿™é‡Œå®é™…ä¸Šå°±åªæœ‰ä¸€ä¸ªCSVæ–‡ä»¶ï¼‰
        è¿›è¡Œæ¸…æ´—ï¼Œåªä¿ç•™ "DOI", "æ¥æº", "ç ”ç©¶æ–¹å‘", "è®ºæ–‡æ ‡é¢˜" 4åˆ—ï¼Œ
        å¹¶åˆå¹¶ä¸ºä¸€ä¸ª DataFrame è¿”å›
        """
        all_dfs = []
        for file in os.listdir(self.input_dir):
            if not file.endswith(".csv"):
                continue
            try:
                df = pd.read_csv(os.path.join(self.input_dir, file), encoding="utf-8-sig")
                all_dfs.append(df)
            except Exception as e:
                print(f"âš ï¸ æ–‡ä»¶ {file} è¯»å–å¤±è´¥: {e}")

        if not all_dfs:
            raise RuntimeError("âŒ è¾“å…¥ç›®å½•ä¸‹æ²¡æœ‰å¯ç”¨çš„ CSV æ–‡ä»¶")
        
        # åˆå¹¶æ‰€æœ‰çš„ DataFrameï¼Œå®é™…ä¸Šè¿™é‡Œåªæœ‰ä¸€ä¸ª
        df = pd.concat(all_dfs, ignore_index=True)

        # ---------- æœ‰æ—¶å€™ä¸‹è½½ä¸­æ–‡åï¼Œæœ‰æ—¶å€™ä¸‹è½½è‹±æ–‡åï¼Œå®šä¹‰2ç§æ–‡ä»¶ä¸­çš„csvå­—æ®µåç§° ----------
        col_alias = {
            "doi": ["doi", "DOI"],
            "source": ["source", "æ¥æº", "Source"],
            "field": ["research area", "ç ”ç©¶æ–¹å‘", "Research Area"],
            "title": ["article title", "è®ºæ–‡æ ‡é¢˜", "Article Title"],
        }

        cols_lower = [c.strip().lower() for c in df.columns]

        def find_col(keys):
            """ä¸è®ºæ˜¯ä¸­æ–‡è¿˜æ˜¯è‹±æ–‡åˆ—ï¼Œéƒ½å¯ä»¥æ‰¾åˆ°"""
            for k in keys:
                for c in df.columns:
                    if c.strip().lower() == k.lower():
                        return c
            return None

        doi_col = find_col(col_alias["doi"])
        source_col = find_col(col_alias["source"])
        field_col = find_col(col_alias["field"])
        title_col = find_col(col_alias["title"])

        # åˆ¤æ–­æ–‡ä»¶æ˜¯å¦ç¼ºå°‘æŸä¸ªåˆ—
        missing = [k for k, v in {
            "DOI": doi_col,
            "Source/æ¥æº": source_col,
            "Research Area/ç ”ç©¶æ–¹å‘": field_col,
            "Article Title/è®ºæ–‡æ ‡é¢˜": title_col,
        }.items() if v is None]

        if missing:
            raise RuntimeError(f"âŒ ç¼ºå°‘å¿…è¦åˆ—: {missing}\nå½“å‰åˆ—å: {list(df.columns)}")

        # æ¸…æ´— DOI ï¼Œå»é‡ï¼Œå­˜åœ¨/ï¼Œå»ç©º 
        df[doi_col] = df[doi_col].astype(str).str.strip()
        df = df.dropna(subset=[doi_col])
        df = df[df[doi_col].str.contains("/", na=False)]
        df = df.drop_duplicates(subset=[doi_col]).reset_index(drop=True)

        # æ•°æ®æ¸…æ´—ï¼Œåªä¿ç•™åŸå§‹æ•°æ®çš„4ä¸ªåˆ—å¹¶ç»Ÿä¸€æˆ4ä¸ªå­—æ®µ
        self.df_raw = df[[doi_col, source_col, field_col, title_col]]
        self.df_raw.columns = ["DOI", "æ¥æº", "ç ”ç©¶æ–¹å‘", "è®ºæ–‡æ ‡é¢˜"]  # ç»Ÿä¸€ä¸­æ–‡è¡¨å¤´
        return self.df_raw

    # ================================ Step 2. Crossref æŠ“å– ================================
    @staticmethod
    def get_crossref_metadata(doi: str) -> dict:
        """
        è¾“å…¥å•ç¯‡è®ºæ–‡ DOIï¼Œè¿”å›ä»¥ DOI ä¸ºé”®ï¼Œå€¼ä¸ºåŒ…å«æ‘˜è¦ã€ä½œè€…æœºæ„ã€å‚è€ƒæ–‡çŒ®DOIçš„å­—å…¸
        """
        base_url = "https://api.crossref.org/works/"
        url = f"{base_url}{doi}"
        try:
            headers = {"User-Agent": "Mozilla/5.0 (mailto:3170529323@qq.com)"}
            response = requests.get(url, headers=headers, timeout=25)
            response.raise_for_status()
            msg = response.json().get("message", {})

            abstract_txt = CrossrefMetaProcessor._strip_tags(msg.get("abstract", ""))

            # ä½œè€…åŠæœºæ„
            author_info = []
            for a in msg.get("author", []) or []:
                name = (a.get("given", "") + " " + a.get("family", "")).strip()
                affs = [aff.get("name") for aff in a.get("affiliation", []) if aff.get("name")]
                if name or affs:
                    author_info.append({"name": name, "affiliation": affs})

            # å‚è€ƒæ–‡çŒ® DOI
            ref_dois = []
            for ref in msg.get("reference", []) or []:
                doi_ref = ref.get("DOI") or ref.get("doi")
                if doi_ref:
                    ref_dois.append(doi_ref.strip())

            return {
                "DOI": doi,
                "CR_æ‘˜è¦": abstract_txt,
                "CR_ä½œè€…å’Œæœºæ„": json.dumps(author_info, ensure_ascii=False),
                "CR_å‚è€ƒæ–‡çŒ®DOI": json.dumps(ref_dois, ensure_ascii=False)
            }

        except Exception:
            return {
                "DOI": doi,
                "CR_æ‘˜è¦": "",
                "CR_ä½œè€…å’Œæœºæ„": "[]",
                "CR_å‚è€ƒæ–‡çŒ®DOI": "[]"
            }

    # ================================ Step 3. OpenAlex æŠ“å– ================================
    def get_openalex_topic(self, doi: str) -> dict:
        """
        è¾“å…¥å•ç¯‡è®ºæ–‡ DOIï¼Œè¿”å›ä»¥ OpenAlex_field_list å’Œ OpenAlex_subfield_list ä¸ºé”®çš„å­—å…¸
        """
        base = "https://api.openalex.org/works/"
        url = f"{base}https://doi.org/{doi}"
        headers = {"User-Agent": f"OpenAlex-Client (mailto:{self.openalex_email})"}

        # å•ç¯‡è®ºæ–‡æœ€å¤šå°è¯•3æ¬¡ï¼Œå¦åˆ™è¿”å›ç©ºå€¼
        for attempt in range(3):
            try:
                time.sleep(random.uniform(0.3, 0.7))
                r = requests.get(url, headers=headers, timeout=20)
                
                if r.status_code == 429:
                    time.sleep(3 * (attempt + 1))
                    continue
                if r.status_code == 404:
                    return {"OpenAlex_field_list": [], "OpenAlex_subfield_list": []}

                r.raise_for_status()
                data = r.json()

                topics = data.get("topics") or []
                primary = data.get("primary_topic") or None

                field_set, subfield_set = set(), set()
                def get_name(obj: Any):
                    return obj.get("display_name") if isinstance(obj, dict) else str(obj)

                for t in topics:
                    f, s = get_name(t.get("field")), get_name(t.get("subfield"))
                    if f: field_set.add(f)
                    if s: subfield_set.add(s)

                if primary:
                    f, s = get_name(primary.get("field")), get_name(primary.get("subfield"))
                    if f: field_set.add(f)
                    if s: subfield_set.add(s)

                return {
                    "OpenAlex_field_list": list(field_set),
                    "OpenAlex_subfield_list": list(subfield_set)
                }

            except Exception:
                time.sleep(2)
        return {"OpenAlex_field_list": [], "OpenAlex_subfield_list": []}

    # ================ Step 4. åˆå¹¶ + å¢é‡æ›´æ–°ï¼ˆåŸºäºåŸå§‹æ•°æ®ä¸å½“å‰æ•°æ®çš„å·®é›†ï¼‰=============
    def merge_metadata_with_crossref(self, limit: int = 500):
        """
        æ€»æµç¨‹ï¼š
        1. è¯»å–å½“å‰æ•°æ®å’ŒåŸå§‹æ•°æ®ï¼Œè®¡ç®— DOI å·®é›†ï¼Œå¾—åˆ°æ–°å¢ DOI åˆ—è¡¨
        2. å¯¹æ–°å¢ DOI åˆ—è¡¨ï¼ŒæŠ“å– Crossref å’Œ OpenAlex å…ƒæ•°æ®
        3. åˆå¹¶æ–°å¢ DOI çš„å…ƒæ•°æ®ä¸å½“å‰æ•°æ®çš„å…¶ä»–åˆ—ï¼Œå¾—åˆ°æ–°å¢ DOI çš„å®Œæ•´æ•°æ®
        4. è¿½åŠ åˆ°ç›®æ ‡æ–‡ä»¶ï¼ˆ02crossref_data/<å­¦ç§‘>.csvï¼‰â†’ å»é‡ â†’ è¦†ç›–ä¿å­˜
        """

        # è¯»å–å½“å‰çš„æ•°æ®ï¼ˆæœ‰å¯èƒ½æ¯”åŸå§‹æ•°æ®æœ‰æ‰€å¢åŠ ï¼‰ï¼Œå¹¶è½¬æ¢æˆdfæ ¼å¼ï¼Œ ["DOI","æ¥æº","ç ”ç©¶æ–¹å‘","è®ºæ–‡æ ‡é¢˜"]
        if self.df_raw is None:
            self.load_all_csvs()

        if not self.origin_dir or not os.path.isdir(self.origin_dir):
            raise RuntimeError(f"âŒ æœªæä¾›æœ‰æ•ˆçš„ origin_dirï¼ˆ01origin_meta_data å­¦ç§‘ç›®å½•ï¼‰ï¼š{self.origin_dir}")

        # è¯»å–åŸå§‹çš„æ•°æ®ï¼Œå¹¶ç»Ÿè®¡å…¶æ•°é‡
        origin_csvs = [f for f in os.listdir(self.origin_dir) if f.lower().endswith(".csv")]
        if len(origin_csvs) == 0:
            raise RuntimeError(f"âŒ {self.origin_dir} ä¸‹æ²¡æœ‰ CSV æ–‡ä»¶")
        if len(origin_csvs) > 1:
            raise RuntimeError(f"âŒ {self.origin_dir} ä¸‹æœ‰ {len(origin_csvs)} ä¸ª CSVï¼ŒæœŸæœ›ä»… 1 ä¸ª")
        origin_csv = os.path.join(self.origin_dir, origin_csvs[0])

        def _read_csv_any(path):
            # è¿”å›åŸå§‹æ•°æ®çš„dfæ ¼å¼
            for enc in ("utf-8-sig", "utf-8", "gb18030"):
                try:
                    return pd.read_csv(path, encoding=enc)
                except Exception:
                    pass
            raise RuntimeError(f"âŒ æ–‡ä»¶è¯»å–å¤±è´¥ï¼š{path}")

        df_origin_raw = _read_csv_any(origin_csv)

        # å¤ç”¨åŸå…ˆçš„åˆ—åè¯†åˆ«é€»è¾‘
        col_alias = {
            "doi": ["doi", "DOI"],
            "source": ["source", "æ¥æº", "Source"],
            "field": ["research area", "ç ”ç©¶æ–¹å‘", "Research Area"],
            "title": ["article title", "è®ºæ–‡æ ‡é¢˜", "Article Title"],
        }

        def _find_col(df, keys):
            for k in keys:
                for c in df.columns:
                    if c.strip().lower() == k.lower():
                        return c
            return None

        # åˆ†åˆ«è·å–å½“å‰æ•°æ®å’ŒåŸå§‹æ•°æ®çš„ DOI åˆ—
        doi_col_01 = [c for c in self.df_raw.columns if c.strip().lower() == "doi"][0]
        doi_col_00 = _find_col(df_origin_raw, col_alias["doi"])
        if doi_col_00 is None:
            raise RuntimeError(f"âŒ 00ç›®å½• CSV ç¼ºå°‘ DOI åˆ—ï¼š{origin_csv}\nåˆ—å: {list(df_origin_raw.columns)}")

        # æ¸…æ´—åŸå§‹æ•°æ®çš„ DOI
        df_origin_raw[doi_col_00] = df_origin_raw[doi_col_00].astype(str).str.strip()
        df_origin_raw = df_origin_raw.dropna(subset=[doi_col_00])
        df_origin_raw = df_origin_raw[df_origin_raw[doi_col_00].str.contains("/", na=False)]
        df_origin_raw = df_origin_raw.drop_duplicates(subset=[doi_col_00]).reset_index(drop=True)

        # æ±‚æ–°å¢çš„æ•°æ®çš„ DOI é›†åˆ
        dois_01 = set(self.df_raw[doi_col_01].astype(str))
        dois_00 = set(df_origin_raw[doi_col_00].astype(str))

        new_dois = sorted(list(dois_01 - dois_00))
        if len(new_dois) == 0:
            # å³ä¾¿æ–°å¢ä¸º 0ï¼Œä¹Ÿè¦ä¿è¯ç›®æ ‡æ–‡ä»¶å­˜åœ¨ï¼›è‹¥å·²å­˜åœ¨åˆ™ä¸åŠ¨ï¼Œè‹¥ä¸å­˜åœ¨å¯ä»¥ç›´æ¥è¿”å›
            file_name = os.path.basename(self.input_dir.rstrip("/")) + ".csv"
            out_path = os.path.join(self.output_dir, file_name)
            print("âœ… æ–°å¢ DOI æ•°é‡ä¸º 0ï¼Œè·³è¿‡æŠ“å–ã€‚")
            return out_path

        # é™åˆ¶å¯¹äºæ–°å¢çš„æ•°æ®ï¼ŒåªæŠ“å–å‰ limit æ¡
        if len(new_dois) > limit:
            new_dois = new_dois[:limit]
        print(f"ğŸ” åŸºäº 01-00 å·®é›†ï¼Œéœ€æŠ“å–æ–°å¢ DOI æ•°é‡ï¼š{len(new_dois)}")

        # ä»å½“å‰æ•°æ®ä¸­ï¼ˆdf_rawï¼‰è·å–å…¶ä»–çš„å…ƒæ•°æ®ï¼ˆæ¥æº/ç ”ç©¶æ–¹å‘/æ ‡é¢˜ç­‰ï¼‰
        df_new_input = self.df_raw[self.df_raw[doi_col_01].isin(new_dois)].reset_index(drop=True)

        # ======== å¯¹æ–°å¢æ•°æ®è¿›è¡Œ Openalxe å’Œ CrossRef æ•°æ®çš„è¡¥å…… ========
        from concurrent.futures import ThreadPoolExecutor, as_completed  # é¿å…é¡¶éƒ¨é‡å¤å¯¼å…¥æŠ¥é”™
        records = []

        def safe_fetch(doi: str) -> Dict[str, Any]:
            # å¯¹æ–°å¢çš„ DOI è¿›è¡ŒæŠ“å–
            record = self.get_crossref_metadata(doi)
            openalex_data = self.get_openalex_topic(doi)
            record.update(openalex_data)
            return record

        with ThreadPoolExecutor(max_workers=10) as executor:
            futures = {executor.submit(safe_fetch, doi): doi for doi in df_new_input[doi_col_01]}
            for future in tqdm(as_completed(futures), total=len(futures), desc="Fetching Data", ncols=90):
                try:
                    records.append(future.result())
                except Exception:
                    records.append({
                        "DOI": futures[future],
                        "CR_æ‘˜è¦": "",
                        "CR_ä½œè€…å’Œæœºæ„": "[]",
                        "CR_å‚è€ƒæ–‡çŒ®DOI": "[]",
                        "OpenAlex_field_list": [],
                        "OpenAlex_subfield_list": [],
                        "OpenAlex_map_subjects": []
                    })

        # åˆå¹¶æ–°å¢ DOI çš„æŠ“å–ç»“æœä¸æ–°å¢ DOI çš„å…¶ä»–åˆ—ï¼Œå¾—åˆ°æœ€ç»ˆçš„æ–°å¢ DOI çš„å®Œæ•´æ•°æ®
        df_crossref = pd.DataFrame(records)
        self.df_merged = pd.merge(df_new_input, df_crossref, left_on=doi_col_01, right_on="DOI", how="left")
        self.df_merged = self.df_merged[self.df_merged["CR_æ‘˜è¦"].fillna("").str.strip() != ""]

        # æ˜ å°„è¡¨åŠ è½½
        openalex_to_cn = {}
        if self.map_path.exists():
            with open(self.map_path, "r", encoding="utf-8") as f:
                openalex_to_cn = json.load(f)
            print(f"âœ… å·²åŠ è½½æ˜ å°„è¡¨ï¼š{self.map_path}")
        else:
            print(f"âš ï¸ æœªæ‰¾åˆ°æ˜ å°„æ–‡ä»¶: {self.map_path}")

        # å°† OpenAlex çš„ field ä¸ subfield 2åˆ—æ˜ å°„ä¸ºä¸­å›½å­¦ç§‘å’Œå¯¹åº”çš„åˆ†æ•°
        def map_to_cn_groups(fields: List[str], subfields: List[str]) -> List[List[tuple]]:
            groups = []
            for name in (fields or []) + (subfields or []):
                mapped_pairs = openalex_to_cn.get(name, [])
                clean_pairs = []
                for m in mapped_pairs:
                    if isinstance(m, (list, tuple)) and len(m) == 2:
                        subj, score = m
                        try:
                            clean_pairs.append((str(subj), float(score)))
                        except Exception:
                            continue
                groups.append(clean_pairs)
            return groups

        self.df_merged["OpenAlex_map_subjects"] = self.df_merged.apply(
            lambda r: map_to_cn_groups(r["OpenAlex_field_list"], r["OpenAlex_subfield_list"]),
            axis=1
        )

        # å»æ‰ OpenAlex_field_list å’Œ OpenAlex_subfield_list éƒ½ä¸ºç©ºçš„è®°å½•
        def is_empty_list_or_none(x):
            if x is None:
                return True
            if isinstance(x, str) and x.strip() in ("", "[]", "nan", "None"):
                return True
            if isinstance(x, (list, tuple, set)) and len(x) == 0:
                return True
            return False

        before = len(self.df_merged)
        self.df_merged = self.df_merged[
            ~(
                self.df_merged["OpenAlex_field_list"].apply(is_empty_list_or_none)
                & self.df_merged["OpenAlex_subfield_list"].apply(is_empty_list_or_none)
            )
        ].reset_index(drop=True)
        after = len(self.df_merged)
        print(f"ğŸ§¹ è¿‡æ»¤æ— å­¦ç§‘æ•°æ®: {before - after} æ¡ï¼Œå‰©ä½™ {after} æ¡ã€‚")

        # ======== è¿½åŠ åˆ°ç›®æ ‡æ–‡ä»¶ï¼ˆ02crossref_data/<å­¦ç§‘>.csvï¼‰â†’ å»é‡ â†’ è¦†ç›–ä¿å­˜ ========
        file_name = os.path.basename(self.input_dir.rstrip("/")) + ".csv"
        out_path = os.path.join(self.output_dir, file_name)

        # è¯»å–ç›®æ ‡æ–‡ä»¶çš„æ—§æ•°æ®ï¼ˆå¦‚æœå­˜åœ¨ï¼‰ï¼Œå¹¶æŠŠæ–°å¢çš„ DOI å®Œæ•´æ•°æ®è¿½åŠ è¿›å»ï¼Œå»é‡
        df_old = None
        if os.path.exists(out_path):
            try:
                df_old = pd.read_csv(out_path)
                print(f"ğŸ” æ£€æµ‹åˆ°ç›®æ ‡å†å²æ•°æ®: {len(df_old)} æ¡ï¼Œå°†æ‰§è¡Œè¿½åŠ å¹¶å»é‡...")
            except Exception as e:
                print(f"âš ï¸ è¯»å–æ—§æ–‡ä»¶å¤±è´¥ï¼Œå°†ç›´æ¥åˆ›å»ºæ–°æ–‡ä»¶: {e}")

        if df_old is not None:
            # ç¡®ä¿æ–°å¢ DOI æ•°æ®çš„åˆ—ä¸æ—§æ•°æ®åˆ—ä¸€è‡´ï¼Œå¹¶å°†æ–°çš„ DOI å®Œæ•´æ•°æ®è¿½åŠ è¿›å»
            for col in self.df_merged.columns:
                if col not in df_old.columns:
                    df_old[col] = None
            for col in df_old.columns:
                if col not in self.df_merged.columns:
                    self.df_merged[col] = None
            df_all = pd.concat([df_old, self.df_merged], ignore_index=True)
        else:
            df_all = self.df_merged

        # æŒ‰ç…§ DOI å»é‡å¹¶ä¿å­˜åˆ°åŸå§‹æ–‡ä»¶ä¸­
        df_all = df_all.drop_duplicates(subset=["DOI"]).reset_index(drop=True)
        os.makedirs(self.output_dir, exist_ok=True)
        df_all.to_csv(out_path, index=False, encoding="utf-8-sig")
        print(f"âœ… å·²ä¿å­˜: {out_path} | æ€»è®°å½•æ•°: {len(df_all)}")

        return out_path


    # ================================ Step 5. ç»Ÿè®¡ ================================
    def print_statistics(self):
        if self.df_merged is None or len(self.df_merged) == 0:
            print("âš ï¸ æ— æœ‰æ•ˆè®°å½•ã€‚")
            return
        total = len(self.df_merged)
        no_abs = (self.df_merged["CR_æ‘˜è¦"] == "").sum()
        print("ğŸ“Š ç»Ÿè®¡ä¿¡æ¯")
        print(f"- æ€»è®ºæ–‡æ•°: {total}")
        print(f"- ç¼ºå¤±æ‘˜è¦: {no_abs} ({no_abs/total:.1%})")


if __name__ == "__main__":
    processor = CrossrefMetaProcessor(
        input_dir="data/01meta_data/0101 Philosophy",
        origin_dir="data/01origin_meta_data/0101 Philosophy",   # â† 00ç›®å½•åŒåå­¦ç§‘
        output_dir="data/02crossref_data"
    )
    processor.merge_metadata_with_crossref(limit=8000)
    processor.print_statistics()
