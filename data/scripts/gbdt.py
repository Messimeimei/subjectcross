# -*- coding: utf-8 -*-
# Created by Messimeimei
# Rebuilt by ChatGPT â€” Field-level 3-class GBDT (2025/12)

import os
import json
import ast
import numpy as np
import pandas as pd
from pathlib import Path

from sklearn.svm import SVC
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import classification_report, confusion_matrix, log_loss

import matplotlib.pyplot as plt


# ==========================================================
# å·¥å…·å‡½æ•°ï¼šå®‰å…¨è§£æ list/json å­—æ®µ
# ==========================================================
def safe_parse_list(s):
    if not isinstance(s, str) or s.strip() == "":
        return []
    try:
        return json.loads(s)
    except:
        try:
            return ast.literal_eval(s)
        except:
            return []


# ä» "1205 ä¿¡æ¯èµ„æºç®¡ç†" / "1205" / "['1205',0.7]" ä¸­æŠ½å– 4 ä½ç 
def extract_subject_code(field_name):
    if field_name is None:
        return ""
    s = str(field_name)
    digits = "".join([c for c in s if c.isdigit()])
    return digits[:4] if len(digits) >= 4 else ""


# è§£æäº”ç»´åº¦å­—æ®µï¼š[(å­¦ç§‘å, åˆ†æ•°)] â†’ [(code, score)]
def clean_dim_items(raw):
    cleaned = []
    for item in raw:
        if not isinstance(item, (list, tuple)) or len(item) < 2:
            continue
        code = extract_subject_code(item[0])
        if code == "":
            continue
        try:
            score = float(item[1])
        except:
            continue
        cleaned.append((code, score))
    return cleaned


# ==========================================================
# Part A â€”â€” æ„å»º 5dims_dataset.csv
# ==========================================================
def build_5dims_dataset(
    test_data_file: str,
    predicted_data_file: str,
    output_file: str = "../../data/5dims_dataset.csv"
):
    print("ğŸ“¥ è¯»å– test_data.csvï¼ˆçœŸå®æ ‡ç­¾ï¼‰...")
    test_df = pd.read_csv(test_data_file, dtype=str).fillna("")

    print("ğŸ“¥ è¯»å– predicted_result.csvï¼ˆå«äº”ç»´å­—æ®µï¼‰...")
    pred_df = pd.read_csv(predicted_data_file, dtype=str).fillna("")

    if "DOI" not in test_df.columns or "DOI" not in pred_df.columns:
        raise ValueError("test_data.csv ä¸ predicted_result.csv å¿…é¡»åŒ…å« DOI å­—æ®µ")

    print("ğŸ”„ æŒ‰ DOI åˆå¹¶æ•°æ® ...")
    merged = test_df.merge(pred_df, on="DOI", how="left", suffixes=("", "_pred"))

    required = [
        "DOI", "æ¥æº", "ç ”ç©¶æ–¹å‘", "è®ºæ–‡æ ‡é¢˜", "CR_æ‘˜è¦",
        "CR_ä½œè€…å’Œæœºæ„", "CR_å‚è€ƒæ–‡çŒ®DOI",
        "list_incites_direction", "list_title_abs",
        "list_author_aff_qwen", "list_openalex", "list_ref",
        "primary", "cross"
    ]
    for col in required:
        if col not in merged.columns:
            merged[col] = ""

    new_df = merged[required].copy()

    os.makedirs(os.path.dirname(output_file), exist_ok=True)
    new_df.to_csv(output_file, index=False, encoding="utf-8-sig")

    print(f"ğŸ‰ 5dims_dataset.csv å·²ç”Ÿæˆ/æ›´æ–°ï¼Œå…± {len(new_df)} æ¡")
    return new_df


# ==========================================================
# Part B â€”â€” è½¬æˆ paper_data
# ==========================================================
def convert_csv_to_paper_data(csv_path):
    print(f"\nğŸ“¥ åŠ è½½ 5dims_dataset.csv: {csv_path}")
    df = pd.read_csv(csv_path, dtype=str).fillna("")
    papers = []

    for _, row in df.iterrows():
        dims = {
            "incites": clean_dim_items(safe_parse_list(row["list_incites_direction"])),
            "title_abs": clean_dim_items(safe_parse_list(row["list_title_abs"])),
            "author_aff": clean_dim_items(safe_parse_list(row["list_author_aff_qwen"])),
            "openalex": clean_dim_items(safe_parse_list(row["list_openalex"])),
            "refs": clean_dim_items(safe_parse_list(row["list_ref"])),
        }

        main = [
            extract_subject_code(x)
            for x in row["primary"].replace("ï¼›", ";").split(";")
            if extract_subject_code(x) != ""
        ]
        cross = [
            extract_subject_code(x)
            for x in row["cross"].replace("ï¼›", ";").split(";")
            if extract_subject_code(x) != ""
        ]

        papers.append({
            "paper_id": row["DOI"],
            "dims": dims,
            "label": {
                "main": main,
                "cross": cross,
            }
        })

    print(f"ğŸ“¦ è½¬æ¢å®Œæˆï¼Œå…± {len(papers)} ç¯‡è®ºæ–‡")
    return papers


# ==========================================================
# Part C â€”â€” è®¡ç®—äº”ç»´åº¦å…¨å±€ min/max
# ==========================================================
def compute_global_min_max(paper_data):
    dim_names = ["incites", "title_abs", "author_aff", "openalex", "refs"]
    stats = {d: [] for d in dim_names}

    for p in paper_data:
        for d in dim_names:
            stats[d].extend([float(v) for _, v in p["dims"][d]])

    global_stats = {}
    print("\nğŸ“Š å…¨å±€ min/maxï¼š")
    for d, vals in stats.items():
        if len(vals) == 0:
            global_stats[d] = (0.0, 1.0)
        else:
            global_stats[d] = (min(vals), max(vals))
        print(f" - {d}: {global_stats[d]}")
    return global_stats


def normalize_dim_with_stats(dim_list, mn, mx):
    if not dim_list:
        return {}
    if mn == mx:
        return {f: 0.0 for f, _ in dim_list}
    mn = 0
    return {f: (float(s) - mn) / (mx - mn) for f, s in dim_list}


# ==========================================================
# Part D â€”â€” æ„å»ºå­¦ç§‘çº§åˆ« 3 åˆ†ç±»æ•°æ®ï¼ˆæœ€æ ¸å¿ƒéƒ¨åˆ†ï¼‰
# ==========================================================
def build_dataset_3class(paper_data, stats, stage="train"):

    X_all, y_all, samples = [], [], []

    for p in paper_data:

        # äº”ç»´å½’ä¸€åŒ–
        inc = normalize_dim_with_stats(p["dims"]["incites"], *stats["incites"])
        tit = normalize_dim_with_stats(p["dims"]["title_abs"], *stats["title_abs"])
        aut = normalize_dim_with_stats(p["dims"]["author_aff"], *stats["author_aff"])
        ope = normalize_dim_with_stats(p["dims"]["openalex"], *stats["openalex"])
        ref = normalize_dim_with_stats(p["dims"]["refs"], *stats["refs"])


        # äº”ç»´åº¦ä¸­å‡ºç°çš„æ‰€æœ‰å­¦ç§‘
        fields = set(inc) | set(tit) | set(aut) | set(ope) | set(ref)

        # å¼ºåˆ¶åŠ å…¥ä¸»å­¦ç§‘ä¸äº¤å‰å­¦ç§‘ï¼ˆé¿å…ç¼ºå¤±ï¼‰
        main = p["label"]["main"]
        cross = set(p["label"]["cross"])


        # æ„å»ºå­¦ç§‘æ ·æœ¬
        for f in fields:
            feat = [
                inc.get(f, 0.0),
                tit.get(f, 0.0),
                aut.get(f, 0.0),
                ope.get(f, 0.0),
                ref.get(f, 0.0),
            ]

            if f in main:
                y = 2
            elif f in cross:
                y = 1
            else:
                y = 0

            X_all.append(feat)
            y_all.append(y)
            samples.append({
                "paper_id": p["paper_id"],
                "field": f,
                "feat": feat,
                "label": y,
            })

    print(f"\nğŸ“˜ [{stage}] ä¸‰åˆ†ç±»æ ·æœ¬æ•°ï¼š{len(X_all)}")

    if stage == "train":
        print("\n====== ğŸ”ã€è®­ç»ƒæ ·æœ¬ç¤ºä¾‹ï¼ˆå‰ 10 æ¡ï¼‰ã€‘======")
        for s in samples[:10]:
            print(f"{s['paper_id']} | field={s['field']} | y={s['label']} | feat={s['feat']}")

        y_np = np.array(y_all)
        print("\n====== ğŸ”¢ã€è®­ç»ƒé›†ç±»åˆ«åˆ†å¸ƒã€‘======")
        print(f"0ï¼ˆæ— å…³ï¼‰: {np.sum(y_np==0)}")
        print(f"1ï¼ˆäº¤å‰ï¼‰: {np.sum(y_np==1)}")
        print(f"2ï¼ˆä¸»å­¦ç§‘ï¼‰: {np.sum(y_np==2)}")
    
    print(f"è®­ç»ƒæ ·æœ¬ï¼š{X_all[:10], y_all[:10]}")

    return np.array(X_all), np.array(y_all), samples


# ==========================================================
# Part E â€”â€” è®­ç»ƒ GBDT ä¸‰åˆ†ç±»
# ==========================================================
def train_gbdt(X, y):

    # print("\nğŸ”¥ ä½¿ç”¨ RBF-SVM è¿›è¡Œ 3 åˆ†ç±»è®­ç»ƒ ...")

    # model = SVC(
    #     kernel="rbf",
    #     probability=True,   # å¿…é¡»æ‰“å¼€æ‰èƒ½ predict_proba
    #     C=2.0,
    #     gamma="scale",
    #     class_weight="balanced"   # â­ æ ¸å¿ƒä¿®å¤

    # )

    # model.fit(X, y)

    # print("ğŸ‰ RBF-SVM è®­ç»ƒå®Œæˆ")
    # return model

    print("\nğŸ”¥ ä½¿ç”¨ GBDT è®­ç»ƒä¸‰åˆ†ç±»æ¨¡å‹ ...")

    model = GradientBoostingClassifier(
        n_estimators=100,
        learning_rate=0.05,
        max_depth=3,
        subsample=0.8,         # ä½¿ç”¨éƒ¨åˆ†æ ·æœ¬ï¼ˆæ›´ç¨³å®šï¼‰

    )
    model.fit(X, y)

    print("ğŸ‰ GBDT è®­ç»ƒå®Œæˆ")

    print("\n====== ğŸ“‰ã€è®­ç»ƒ Lossã€‘======")
    for i, y_pred in enumerate(model.staged_predict_proba(X)):
        if i % 50 == 0:
            loss = log_loss(y, y_pred, labels=[0,1,2])
            print(f"Iter {i:3d} | loss={loss:.4f}")

    return model


# ==========================================================
# Part F â€”â€” å­¦ç§‘çº§åˆ«è¯„ä¼°ï¼ˆä½ è¦çš„ç»“æœï¼‰
# ==========================================================
def evaluate_field_level_3class(y_true, y_pred):
    print("\n====== ğŸ“Šã€å­—æ®µçº§ä¸‰åˆ†ç±»æŠ¥å‘Šã€‘======")
    print(classification_report(y_true, y_pred, digits=4))

    print("\n====== ğŸ“Šã€æ··æ·†çŸ©é˜µã€‘======")
    print(confusion_matrix(y_true, y_pred))


# ==========================================================
# ä¸»å…¥å£
# ==========================================================
if __name__ == "__main__":

    ROOT = Path(__file__).resolve().parents[2]
    DATA_DIR = ROOT / "data"

    test_file = DATA_DIR / "test_data.csv"
    pred_file = DATA_DIR / "predicted_result.csv"
    dims_file = DATA_DIR / "5dims_dataset.csv"

    # Step 1: æ„å»º 5dims_dataset
    build_5dims_dataset(test_file, pred_file, dims_file)

    # Step 2: è§£ææˆ paper_data
    papers = convert_csv_to_paper_data(dims_file)

    # Step 3: è®ºæ–‡çº§åˆ’åˆ†ï¼ˆä¾‹å¦‚ 40 / 10ï¼‰
    idx = np.arange(len(papers))
    np.random.shuffle(idx)
    train_papers = [papers[i] for i in idx[:45]]
    test_papers  = [papers[i] for i in idx[45:]]

    # Step 4: å…¨å±€ min-maxï¼ˆåŸºäºè®­ç»ƒè®ºæ–‡ï¼‰
    stats = compute_global_min_max(train_papers)

    # Step 5: æ„å»ºå­¦ç§‘çº§è®­ç»ƒé›†
    X_train, y_train, train_samples = build_dataset_3class(train_papers, stats, stage="train")

    # Step 6: è®­ç»ƒæ¨¡å‹
    model = train_gbdt(X_train, y_train)

    # Step 7: æ„å»ºå­¦ç§‘çº§æµ‹è¯•é›†
    X_test, y_test, test_samples = build_dataset_3class(test_papers, stats, stage="test")

    # Step 8: æµ‹è¯•è¯„ä¼°ï¼ˆä½ è¦çš„ç»“æœï¼‰
    y_pred = model.predict(X_test)
    evaluate_field_level_3class(y_test, y_pred)

    print("\nğŸ‰ å®Œæˆï¼")
