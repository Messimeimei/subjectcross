# -*- coding: utf-8 -*-
# Created by Messimeimei
# Modified by ChatGPT â€” conversation-style dataset (2025/12)
"""
æœ¬è„šæœ¬æä¾›å®Œæ•´è®­ç»ƒæ•°æ®ç”Ÿæˆæµæ°´çº¿ï¼ˆconversation æ ¼å¼ï¼‰ï¼š

åŸå§‹ç‰ˆæœ¬è¾“å‡ºæ ·æœ¬æ ¼å¼ï¼š
    {
        "instruction": "...",
        "input": "é¢˜åï¼š...\næ‘˜è¦ï¼š...",
        "output": {
            "primary": "...",
            "cross": [...]
        }
    }

æ”¹é€ åè¾“å‡ºä¸ºå¯¹è¯æ ¼å¼ï¼š
    {
        "conversation_id": "xxx",
        "category": "ä¸»/äº¤å‰å­¦ç§‘åˆ¤å®šï¼ˆ117 ä¸€çº§å­¦ç§‘ï¼Œå¤šæ ‡ç­¾ï¼‰",
        "conversation": [
            {
                "human": "<instruction + input>",
                "assistant": "<output çš„ JSON æ–‡æœ¬>"
            }
        ]
    }

æµæ°´çº¿æ­¥éª¤ä¿æŒä¸å˜ï¼š
1. è¯»å– ../05output_data ä¸‹çš„æ¯ä¸ªå­¦ç§‘ CSV
2. ä» CSV ä¸­è¯»å– primary / cross å­—æ®µ
3. è½¬æ¢ä¸º JSONLï¼ˆæ¯å­¦ç§‘ç‹¬ç«‹ï¼Œconversation æ ¼å¼ï¼‰
4. æ¯ä¸ªå­¦ç§‘ç‹¬ç«‹æŒ‰ 0.6 / 0.2 / 0.2 åˆ’åˆ† train/val/test
5. æ±‡æ€»ç”Ÿæˆå…¨å±€ train.jsonl / val.jsonl / test.jsonl
6. è‡ªåŠ¨ç»˜åˆ¶ 4 å¼ å¯è§†åŒ–å›¾ï¼š
   - æ•°æ®é›†è§„æ¨¡æŸ±çŠ¶å›¾
   - ä¸»å­¦ç§‘åˆ†å¸ƒå›¾
   - è¾“å…¥é•¿åº¦åˆ†å¸ƒï¼ˆå­—ç¬¦æ•°ï¼‰
   - Token é•¿åº¦åˆ†å¸ƒ
"""

import os
import json
import random
import pandas as pd
import matplotlib.pyplot as plt
from tqdm import tqdm
from collections import Counter
from transformers import AutoTokenizer

from pathlib import Path
from transformers import AutoTokenizer

# ================================
# å…¨å±€é…ç½®
# ================================

SPLIT_RATIO = (0.6, 0.2, 0.2)
SEED = 42

# ç”¨å½“å‰è„šæœ¬ä½ç½®æ¨æ–­é¡¹ç›®æ ¹ç›®å½•ï¼Œå†å®šä½æœ¬åœ°æ¨¡å‹
THIS_FILE = Path(__file__).resolve()
PROJECT_ROOT = THIS_FILE.parents[2]          # ~/pyprojects/subjectcross
TOKENIZER_PATH = PROJECT_ROOT / "models/base/Qwen2.5-0.5B-Instruct"
tokenizer = AutoTokenizer.from_pretrained(str(TOKENIZER_PATH), trust_remote_code=True)
MAX_CROSS = 3
INPUT_DIR = PROJECT_ROOT / "data/05output_data"
OUTPUT_DIR = PROJECT_ROOT / "data/06finetune_data"
os.makedirs(OUTPUT_DIR, exist_ok=True)


# category å­—æ®µï¼šç»Ÿä¸€å†™æ­»
CATEGORY = "ä¸»/äº¤å‰å­¦ç§‘åˆ¤å®šï¼ˆ117 ä¸€çº§å­¦ç§‘ï¼Œå¤šæ ‡ç­¾ï¼‰"

# ================================
# Prompt æ„é€ ï¼šæ–°ç‰ˆï¼ˆæ— åˆ†æ•°ï¼‰
# ================================
disciplines_df = pd.read_csv(PROJECT_ROOT / "data/zh_disciplines.csv", encoding="utf-8")
discipline_list = (
    disciplines_df.columns.tolist() +
    disciplines_df.iloc[:, 0].tolist()
)



# ================================
# å·¥å…·å‡½æ•°
# ================================
def safe_json_loads(text):
    try:
        return json.loads(text)
    except Exception:
        return None


def write_jsonl(path, data):
    with open(path, "w", encoding="utf-8") as f:
        for item in data:
            f.write(json.dumps(item, ensure_ascii=False) + "\n")


def build_conversation_id(subject: str, idx: int, row: pd.Series) -> str:
    """
    ç”Ÿæˆ conversation_idï¼š
    - ä¼˜å…ˆä½¿ç”¨ DOIï¼š"<DOI>$No<idx>"
    - æ²¡æœ‰ DOI å°±ç”¨ "<subject>$No<idx>"
    """
    doi = str(row.get("DOI", "")).strip()
    if doi:
        return f"{doi}$No{idx}"
    return f"{subject}$No{idx}"


# ================================
# CSV â†’ conversation æ ·æœ¬åˆ—è¡¨
# ================================
def convert_csv(csv_path, subject_name: str):
    """
    ä»å•ä¸ªå­¦ç§‘çš„ CSV æ„é€  conversation æ ·æœ¬ï¼š
    {
        "conversation_id": "...",
        "category": CATEGORY,
        "conversation": [
            {
                "human": INSTRUCTION + "\\n\\n" + "é¢˜åï¼š...\\næ‘˜è¦ï¼š...",
                "assistant": "<output JSON å­—ç¬¦ä¸²>"
            }
        ]
    }
    """
    df = pd.read_csv(csv_path, encoding="utf-8").fillna("")
    samples = []

    for idx, (_, row) in enumerate(
        tqdm(df.iterrows(), total=len(df), desc=f"è§£æ {os.path.basename(csv_path)}")
    ):
        title = row.get("è®ºæ–‡æ ‡é¢˜", "").strip()
        abstract = row.get("CR_æ‘˜è¦", "").strip()

        # --- ç›´æ¥è¯»å– CSV ä¸­çš„ primary / cross å­—æ®µ ---
        primary = row.get("primary", "").strip()
        cross_raw = row.get("cross", "").strip()

        if not primary:
            # æ²¡æœ‰ä¸»å­¦ç§‘çš„æ ·æœ¬è·³è¿‡
            continue

        # cross å­—æ®µï¼šå…¼å®¹ä¸­è‹±æ–‡é€—å·å’Œåˆ†å·
        cross = []
        if cross_raw:
            tmp = (
                cross_raw.replace("ï¼›", ",")
                .replace("ï¼Œ", ",")
                .split(",")
            )
            cross = [c.strip() for c in tmp if c.strip()]

        # ===== æ„é€ åŸå§‹ SFT ä¸‰å…ƒç»„ï¼ˆä»…ä½œä¸ºä¸­é—´å½¢æ€ï¼‰ =====
        # 1ï¼‰human = åŸæ¥çš„ instruction + input
        human_text = (
            "ä½ æ˜¯ä¸€åè®ºæ–‡å­¦ç§‘çš„åˆ†ç±»ä¸“å®¶ï¼Œæ“…é•¿é€šè¿‡ç»™å®šçš„è®ºæ–‡æ ‡é¢˜å’Œæ‘˜è¦ï¼Œåˆ¤æ–­å‡ºè®ºæ–‡çš„ä¸»å­¦ç§‘å’Œæ¶‰åŠçš„äº¤å‰å­¦ç§‘ã€‚"
            "ç°åœ¨ç»™ä½ ä¸€ç¯‡è®ºæ–‡çš„æ ‡é¢˜å’Œæ‘˜è¦ï¼Œè¯·åˆ¤æ–­ï¼š"
            " 1ï¼‰è®ºæ–‡çš„ä¸»å­¦ç§‘ï¼ˆprimaryï¼‰"
            " 2ï¼‰è®ºæ–‡æ¶‰åŠçš„äº¤å‰å­¦ç§‘ï¼ˆcrossï¼‰ï¼Œæœ€å¤š 3 ä¸ªï¼Œå¯ä¸ºç©º\n"
            "ã€è®ºæ–‡ä¿¡æ¯ã€‘\n"
            f"è®ºæ–‡æ ‡é¢˜: {title}\n"
            f"è®ºæ–‡æ‘˜è¦: {abstract}\n"
            "ã€è¾“å‡ºæ ¼å¼è¦æ±‚ã€‘\n"
            "â€¢ è¾“å‡ºæ ¼å¼ä¸º JSONï¼Œä¸èƒ½åŒ…å«é¢å¤–è§£é‡Šã€‚\n"
            "â€¢ primary å¿…é¡»ä¸”åªèƒ½æœ‰ 1 ä¸ªï¼Œæ ¼å¼ï¼š'ä»£ç  å­¦ç§‘å'\n"
            "â€¢ cross æ˜¯æ•°ç»„ï¼Œæ¯ä¸ªå…ƒç´ ä¸º ä»£ç  å­¦ç§‘åï¼ˆæœ€å¤š 3 ä¸ªï¼‰\n"
            "ã€è¾“å‡ºç¤ºä¾‹ã€‘\n"
            "{"
            "'primary': '0812 è®¡ç®—æœºç§‘å­¦ä¸æŠ€æœ¯',"
            "'cross': ['0831 ç”Ÿç‰©åŒ»å­¦å·¥ç¨‹', '0702 ç‰©ç†å­¦']"
            "}"
            )

        output_obj = {
            "primary": primary,
            "cross": cross
        }

        # 3ï¼‰conversation_id & category
        conv_id = build_conversation_id(subject_name, idx, row)

        sample = {
            "conversation_id": conv_id,
            "category": CATEGORY,
            "conversation": [
                {
                    "human": human_text,
                    "assistant": json.dumps(output_obj, ensure_ascii=False) # è½¬æˆå­—ç¬¦ä¸²
                }
            ]
        }
        samples.append(sample)

    return samples


# ================================
# æ¯å­¦ç§‘æ‹†åˆ† train/val/test
# ================================
def split_by_ratio(samples):
    random.seed(SEED)
    random.shuffle(samples)

    n = len(samples)
    n_train = int(n * SPLIT_RATIO[0])
    n_val = int(n * SPLIT_RATIO[1])
    n_test = n - n_train - n_val

    train = samples[:n_train]
    val = samples[n_train:n_train + n_val]
    test = samples[n_train + n_val:]

    return train, val, test


# ================================
# å¯è§†åŒ–å·¥å…·ï¼ˆåŸºäº conversationï¼‰
# ================================
def plot_dataset_statistics(train, val, test):
    # è®¾ç½®å…¨å±€è‹±æ–‡å­—ä½“
    plt.rcParams['font.family'] = 'Arial'
    plt.rcParams['axes.unicode_minus'] = False  # é¿å…è´Ÿå·æ˜¾ç¤ºå¼‚å¸¸

    # å›¾ 1ï¼šæ¯”ä¾‹
    plt.figure(figsize=(6, 5))
    plt.bar(
        ["train", "val", "test"],
        [len(train), len(val), len(test)],
        color=["#4e79a7", "#f28e2c", "#e15759"]
    )
    plt.title("Train / Validation / Test Sample Count")
    plt.ylabel("Number of Samples")
    plt.tight_layout()
    plt.savefig("chart_dataset_split.png", dpi=160)
    plt.close()

    # å›¾ 2ï¼šä¸»å­¦ç§‘åˆ†å¸ƒï¼ˆTop 20ï¼‰
    def extract_primary(sample):
        try:
            assist = sample["conversation"][0]["assistant"]
            obj = safe_json_loads(assist)
            if isinstance(obj, dict):
                return obj.get("primary", "")
        except Exception:
            return ""
        return ""

    primary_list = [extract_primary(x) for x in train]
    primary_list = [p for p in primary_list if p]
    if primary_list:
        counter = Counter(primary_list).most_common(20)
        labels, values = zip(*counter)

        plt.figure(figsize=(10, 7))
        plt.barh(labels[::-1], values[::-1], color="#4e79a7")
        plt.title("Top 20 Primary Disciplines (Train Set)")
        plt.xlabel("Number of Samples")
        plt.tight_layout()
        plt.savefig("chart_primary_distribution.png", dpi=160)
        plt.close()

    # å›¾ 3ï¼šè¾“å…¥æ–‡æœ¬é•¿åº¦ï¼ˆå­—ç¬¦ï¼‰â€”â€”è¿™é‡Œç”¨ humanï¼ˆinstruction+inputï¼‰
    input_lens = [len(x["conversation"][0]["human"]) for x in train]

    plt.figure(figsize=(8, 5))
    plt.hist(input_lens, bins=50, color="#59a14f")
    plt.title("Distribution of Input Text Length (Characters)")
    plt.xlabel("Character Count")
    plt.ylabel("Frequency")
    plt.tight_layout()
    plt.savefig("chart_input_length.png", dpi=160)
    plt.close()

    # å›¾ 4ï¼štoken åˆ†å¸ƒï¼ˆå¯¹ human ç¼–ç ï¼‰
    token_lens = [
        len(tokenizer.encode(x["conversation"][0]["human"]))
        for x in train
    ]

    plt.figure(figsize=(8, 5))
    plt.hist(token_lens, bins=50, color="#af7aa1")
    plt.title("Distribution of Input Token Length")
    plt.xlabel("Token Count")
    plt.ylabel("Frequency")
    plt.tight_layout()
    plt.savefig("chart_input_token_length.png", dpi=160)
    plt.close()

    print("ğŸ‰ Visualization completed (conversation format, English fonts).")


# ================================
# ä¸»æµç¨‹
# ================================
if __name__ == "__main__":
    train_all, val_all, test_all = [], [], []

    for csv_file in os.listdir(INPUT_DIR):
        if not csv_file.endswith(".csv"):
            continue

        subject = csv_file.replace(".csv", "")
        csv_path = os.path.join(INPUT_DIR, csv_file)

        print(f"\n=== å¤„ç†å­¦ç§‘ï¼š{subject} ===")
        samples = convert_csv(csv_path, subject)

        if not samples:
            print(f"âš ï¸ å­¦ç§‘ {subject} æ— æœ‰æ•ˆæ ·æœ¬")
            continue

        train_s, val_s, test_s = split_by_ratio(samples)

        # æ¯å­¦ç§‘ç‹¬ç«‹æ–‡ä»¶ï¼ˆconversation æ ¼å¼ï¼‰
        write_jsonl(os.path.join(OUTPUT_DIR, f"{subject}_train.jsonl"), train_s)
        write_jsonl(os.path.join(OUTPUT_DIR, f"{subject}_val.jsonl"), val_s)
        write_jsonl(os.path.join(OUTPUT_DIR, f"{subject}_test.jsonl"), test_s)

        train_all.extend(train_s)
        val_all.extend(val_s)
        test_all.extend(test_s)

    # å…¨å±€åˆå¹¶ï¼ˆconversation æ ¼å¼ï¼‰
    write_jsonl(os.path.join(OUTPUT_DIR, "train.jsonl"), train_all)
    write_jsonl(os.path.join(OUTPUT_DIR, "val.jsonl"), val_all)
    write_jsonl(os.path.join(OUTPUT_DIR, "test.jsonl"), test_all)

    print("\nğŸ¯ å…¨éƒ¨å®Œæˆï¼ï¼ˆconversation æ ¼å¼ï¼‰")
    print(f"Train: {len(train_all)}   Val: {len(val_all)}   Test: {len(test_all)}")

    # ç”Ÿæˆå¯è§†åŒ–å›¾
    plot_dataset_statistics(train_all, val_all, test_all)
