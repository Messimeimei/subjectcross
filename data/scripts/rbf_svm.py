# -*- coding: utf-8 -*-
# Created by Messimeimei
# Updated with RBF-SVM 3-class classifier (2025/11/26)
"""
äº”ç»´èåˆ â†’ RBF-SVM â†’ 3 åˆ†ç±»ä»»åŠ¡
---------------------------------------
åˆ†ç±»å®šä¹‰ï¼š
    0 = æ— å…³å­¦ç§‘ï¼ˆneitherï¼‰
    1 = äº¤å‰å­¦ç§‘ï¼ˆcrossï¼‰
    2 = ä¸»å­¦ç§‘ï¼ˆmainï¼‰

è¾“å…¥ç‰¹å¾ï¼š
    äº”ç»´èåˆå‘é‡ï¼šincites, title_abs, author_aff, openalex, refs

æ­¥éª¤ï¼š
 1. åˆå¹¶ test_data.csv + predicted_result.csv â†’ 5dims_dataset.csv
 2. è§£æäº”ç»´å­—æ®µ â†’ paper_data
 3. è®¡ç®—å…¨å±€ min/max â†’ å½’ä¸€åŒ–
 4. æ„å»ºä¸‰åˆ†ç±»è®­ç»ƒé›†
 5. RBF-SVM è®­ç»ƒå¤šåˆ†ç±»æ¨¡å‹
 6. æµ‹è¯•é›†é¢„æµ‹ + Accuracy
"""

import os
import json
import ast
import pandas as pd
import numpy as np
from pathlib import Path
from typing import List, Dict, Any, Tuple

from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report
import pickle

# ================================================================
# Part 0 â€”â€” åˆå¹¶æ•°æ®ï¼Œç”Ÿæˆ 5dims_dataset.csv
# ================================================================
def build_5dims_dataset(test_data_file: str,
                        predicted_data_file: str,
                        output_file: str):

    print("ğŸ“¥ è¯»å– test_data.csvï¼ˆçœŸå®æ ‡ç­¾ï¼‰...")
    test_df = pd.read_csv(test_data_file, dtype=str).fillna("")

    print("ğŸ“¥ è¯»å– predicted_result.csvï¼ˆå«äº”ç»´å­—æ®µï¼‰...")
    pred_df = pd.read_csv(predicted_data_file, dtype=str).fillna("")

    if "DOI" not in test_df.columns or "DOI" not in pred_df.columns:
        raise ValueError("test_data.csv ä¸ predicted_result.csv å¿…é¡»åŒ…å« DOI å­—æ®µ")

    print("ğŸ”„ æŒ‰ DOI åˆå¹¶æ•°æ® ...")
    merged = test_df.merge(pred_df, on="DOI", how="left", suffixes=("", "_pred"))

    required = [
        "DOI", "æ¥æº", "ç ”ç©¶æ–¹å‘", "è®ºæ–‡æ ‡é¢˜",
        "CR_æ‘˜è¦", "CR_ä½œè€…å’Œæœºæ„", "CR_å‚è€ƒæ–‡çŒ®DOI",
        "list_incites_direction", "list_title_abs",
        "list_author_aff_qwen", "list_openalex", "list_ref",
        "primary", "cross"
    ]

    for col in required:
        if col not in merged.columns:
            merged[col] = ""

    os.makedirs(os.path.dirname(output_file), exist_ok=True)
    merged[required].to_csv(output_file, index=False, encoding="utf-8-sig")

    print(f"ğŸ‰ 5dims_dataset.csv å·²ç”Ÿæˆï¼Œå…± {len(merged)} æ¡")
    return merged


# ================================================================
# å·¥å…·å‡½æ•°ï¼šäº”ç»´å­—æ®µè§£æ
# ================================================================
def safe_parse_list(s: str):
    if not isinstance(s, str) or s.strip() == "":
        return []
    try:
        return json.loads(s)
    except:
        try:
            return ast.literal_eval(s)
        except:
            return []


def extract_subject_code(field_name: str) -> str:
    if not isinstance(field_name, str) or field_name.strip() == "":
        return ""
    digits = "".join(c for c in field_name if c.isdigit())
    return digits[:4] if len(digits) >= 4 else ""


def clean_dim_items(raw_list):
    cleaned = []
    for item in raw_list:
        if isinstance(item, (list, tuple)) and len(item) >= 2:
            subject_code = extract_subject_code(str(item[0]))
            if subject_code == "":
                continue
            try:
                score = float(item[1])
            except:
                continue
            cleaned.append((subject_code, score))
    return cleaned


# ================================================================
# Part 2 â€”â€” è½¬æ¢ä¸º paper_data
# ================================================================
def convert_csv_to_paper_data(csv_path: str):
    df = pd.read_csv(csv_path, dtype=str).fillna("")
    paper_data = []

    for _, row in df.iterrows():
        dims = {
            "incites": clean_dim_items(safe_parse_list(row["list_incites_direction"])),
            "title_abs": clean_dim_items(safe_parse_list(row["list_title_abs"])),
            "author_aff": clean_dim_items(safe_parse_list(row["list_author_aff_qwen"])),
            "openalex": clean_dim_items(safe_parse_list(row["list_openalex"])),
            "refs": clean_dim_items(safe_parse_list(row["list_ref"])),
        }

        main_label = extract_subject_code(row["primary"])
        cross_codes = [
            extract_subject_code(x)
            for x in row["cross"].split("ï¼›")
            if extract_subject_code(x) != ""
        ]

        paper_data.append({
            "paper_id": row["DOI"].strip(),
            "dims": dims,
            "label": {"main": main_label, "cross": cross_codes},
        })

    print(f"ğŸ“¦ å·²è½¬æ¢ paper_dataï¼Œå…± {len(paper_data)} ç¯‡è®ºæ–‡")
    return paper_data


# ================================================================
# Part 3 â€”â€” å…¨å±€ min / max
# ================================================================
def compute_global_min_max(paper_data):
    dim_names = ["incites", "title_abs", "author_aff", "openalex", "refs"]
    stats = {name: [] for name in dim_names}

    for paper in paper_data:
        dims = paper["dims"]
        for name in dim_names:
            for _, score in dims.get(name, []):
                stats[name].append(float(score))

    global_stats = {}
    print("\nğŸ“Š å…¨å±€ min/maxï¼š")
    for name, values in stats.items():
        if not values:
            global_stats[name] = (0, 1)
            print(f" - {name} = EMPTY")
        else:
            mn, mx = min(values), max(values)
            global_stats[name] = (mn, mx)
            print(f" - {name}: min={mn:.4f}, max={mx:.4f}")

    return global_stats


def normalize_dim_with_stats(dim_list, min_v, max_v):
    if not dim_list:
        return {}
    if min_v == max_v:
        return {f: 0.0 for f, _ in dim_list}
    return {f: (float(s) - min_v) / (max_v - min_v) for f, s in dim_list}


# ================================================================
# Part 4 â€”â€” 3 åˆ†ç±»è®­ç»ƒé›†æ„å»ºï¼ˆæ ¸å¿ƒï¼‰
# ================================================================
def build_dataset_3class(paper_data, global_stats):

    X_all, y_all, paper_index = [], [], []

    for paper in paper_data:
        dims = paper["dims"]

        inc = normalize_dim_with_stats(dims["incites"], *global_stats["incites"])
        tit = normalize_dim_with_stats(dims["title_abs"], *global_stats["title_abs"])
        aut = normalize_dim_with_stats(dims["author_aff"], *global_stats["author_aff"])
        ope = normalize_dim_with_stats(dims["openalex"], *global_stats["openalex"])
        ref = normalize_dim_with_stats(dims["refs"], *global_stats["refs"])

        fields = set(inc) | set(tit) | set(aut) | set(ope) | set(ref)
        if not fields:
            continue

        main = paper["label"]["main"]
        cross = set(paper["label"]["cross"])

        for f in fields:
            X_all.append([
                inc.get(f, 0.0),
                tit.get(f, 0.0),
                aut.get(f, 0.0),
                ope.get(f, 0.0),
                ref.get(f, 0.0),
            ])

            # ---------- ä¸‰åˆ†ç±»æ ‡ç­¾ ----------
            if f == main:
                y = 2
            elif f in cross:
                y = 1
            else:
                y = 0
            # ---------------------------------

            y_all.append(y)
            paper_index.append((paper["paper_id"], f))

    print(f"ğŸ“š è®­ç»ƒæ ·æœ¬æ•°ï¼š{len(X_all)}æ¡")
    return np.array(X_all), np.array(y_all), paper_index


# ================================================================
# Part 5 â€”â€” RBF-SVM ä¸‰åˆ†ç±»æ¨¡å‹
# ================================================================
def train_rbf_svm_3class(X, y):
    print("\nğŸ”¥ ä½¿ç”¨ RBF-SVM è¿›è¡Œ 3 åˆ†ç±»è®­ç»ƒ ...")

    model = SVC(
        kernel="rbf",
        probability=True,   # å¿…é¡»æ‰“å¼€æ‰èƒ½ predict_proba
        C=2.0,
        gamma="scale",
        class_weight="balanced"   # â­ æ ¸å¿ƒä¿®å¤

    )

    model.fit(X, y)

    print("ğŸ‰ RBF-SVM è®­ç»ƒå®Œæˆ")
    return model


# ================================================================
# Part 6 â€”â€” æµ‹è¯•é›†é¢„æµ‹ï¼ˆä¸‰åˆ†ç±»ï¼‰
# ================================================================
def predict_3class(model, test_data, global_stats):
    results = []

    for paper in test_data:
        dims = paper["dims"]

        inc = normalize_dim_with_stats(dims["incites"], *global_stats["incites"])
        tit = normalize_dim_with_stats(dims["title_abs"], *global_stats["title_abs"])
        aut = normalize_dim_with_stats(dims["author_aff"], *global_stats["author_aff"])
        ope = normalize_dim_with_stats(dims["openalex"], *global_stats["openalex"])
        ref = normalize_dim_with_stats(dims["refs"], *global_stats["refs"])

        fields = set(inc) | set(tit) | set(aut) | set(ope) | set(ref)
        if not fields:
            continue

        true_main = paper["label"]["main"]
        true_cross = set(paper["label"]["cross"])

        for f in fields:
            x = np.array([[inc.get(f, 0), tit.get(f, 0),
                           aut.get(f, 0), ope.get(f, 0), ref.get(f, 0)]])
            prob = model.predict_proba(x)[0]
            pred = int(np.argmax(prob))

            real = 2 if f == true_main else (1 if f in true_cross else 0)

            results.append({
                "paper_id": paper["paper_id"],
                "field": f,
                "pred": pred,
                "real": real,
                "prob_0": prob[0],
                "prob_1": prob[1],
                "prob_2": prob[2],
            })

    return results


# ================================================================
# Part 7 â€”â€” ä¸‰åˆ†ç±»å‡†ç¡®ç‡
# ================================================================
def compute_3class_accuracy(results):
    y_true = [r["real"] for r in results]
    y_pred = [r["pred"] for r in results]
    acc = accuracy_score(y_true, y_pred)
    print(f"\nğŸ¯ 3-class Accuracy = {acc:.4f}")
    print("\nğŸ“‹ åˆ†ç±»æŠ¥å‘Šï¼š")
    print(classification_report(y_true, y_pred, digits=4))
    return acc


# ================================================================
# ä¸»å…¥å£
# ================================================================
if __name__ == "__main__":

    ROOT = Path(__file__).resolve().parents[2]
    DATA_DIR = ROOT / "data"

    test_data_file = DATA_DIR / "test_data.csv"
    pred_data_file = DATA_DIR / "predicted_result.csv"
    output_file = DATA_DIR / "5dims_dataset.csv"

    print("\n====== Step Aï¼šç”Ÿæˆ 5dims_dataset.csv ======")
    build_5dims_dataset(str(test_data_file), str(pred_data_file), str(output_file))

    print("\n====== Step Bï¼šè½¬æ¢ä¸º paper_data ======")
    paper_data = convert_csv_to_paper_data(str(output_file))

    # éšæœºåˆ’åˆ†ï¼ˆå¯è°ƒï¼‰
    idx = np.arange(len(paper_data))
    np.random.seed(42)
    np.random.shuffle(idx)

    train_idx = idx[:30]
    test_idx = idx[30:]

    train_data = [paper_data[i] for i in train_idx]
    test_data = [paper_data[i] for i in test_idx]

    print(f"\nğŸ“Œ Train: {len(train_data)} ç¯‡, Test: {len(test_data)} ç¯‡")

    print("\n====== Step Cï¼šè®¡ç®—å…¨å±€ min/max ======")
    global_stats = compute_global_min_max(train_data)

    print("\n====== Step Dï¼šæ„å»ºä¸‰åˆ†ç±»è®­ç»ƒé›† ======")
    X, y, paper_index = build_dataset_3class(train_data, global_stats)

    print("\n====== Step Eï¼šè®­ç»ƒ RBF-SVM ======")
    model = train_rbf_svm_3class(X, y)

    print("\n====== Step Fï¼šé¢„æµ‹æµ‹è¯•é›† ======")
    results = predict_3class(model, test_data, global_stats)

    print("\n====== Step Gï¼šä¸‰åˆ†ç±»å‡†ç¡®ç‡ ======")
    compute_3class_accuracy(results)

    print("\nğŸ‰ ä»»åŠ¡å®Œæˆï¼")
