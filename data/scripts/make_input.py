# -*- coding: utf-8 -*-
"""
ç”Ÿæˆæœ€åçš„è¾“å…¥æ–‡ä»¶ï¼Œåœ¨03openalex_dataçš„åŸºç¡€ä¸Šï¼Œå°†5ä¸ªç»´åº¦çš„åŸå§‹æ•°æ®ï¼Œå…¨éƒ¨è½¬æ¢æˆå•ä¸ªåˆ—è¡¨ [(å­¦ç§‘ï¼Œåˆ†æ•°)ï¼Œ ...0]çš„å½¢å¼ 
è¾“å‡ºè‡³04input_dataç›®å½•ä¸‹ï¼Œå…·ä½“åŒ…å«ä¸‹åˆ—5ä¸ªç»´åº¦ï¼Œ æ¯ä¸ªç»´åº¦çš„åˆ†æ•°æ¥æºæ–¹å¼ä¸åŒï¼š
- list_incites_directionï¼šç ”ç©¶æ–¹å‘æ˜ å°„ç»“æœï¼Œç›´æ¥æŒ‰ç…§ 1/å­¦ç§‘æ•°é‡ ä¸ºæ¯ä¸ªå­¦ç§‘èµ‹åˆ†
- list_title_absï¼š
    - ç¬¬ä¸€ç§æ–¹å¼ï¼šæ ‡é¢˜+æ‘˜è¦é€šè¿‡bg3-m3è®¡ç®—ç›¸ä¼¼åº¦å–TopNå­¦ç§‘
    - ç¬¬äºŒç§æ–¹å¼ï¼šæ ‡é¢˜+æ‘˜è¦é€šè¿‡bg3-m3è®¡ç®—ç›¸ä¼¼åº¦å–TopNå­¦ç§‘ï¼Œå†é€šè¿‡Qwenæ¨¡å‹è¿›è¡Œåˆ¤å®šå–æœ€å¤š3ä¸ªå­¦ç§‘
    - ç¬¬ä¸‰ç§æ–¹å¼ï¼šæ ‡é¢˜+æ‘˜è¦é€šè¿‡Qwençš„embeddingæ¨¡å‹è®¡ç®—ç›¸ä¼¼åº¦ï¼Œå–TopNå­¦ç§‘è¾“å‡ºï¼Œæ­¤æ—¶çš„TopNå­¦ç§‘æ•°é‡æ›´å¤š
- list_author_aff_qwenï¼šä½œè€…+æœºæ„ â†’ Qwenåˆ¤å®šï¼ˆæ¯æœºæ„æœ€å¤š2ä¸ªå­¦ç§‘ï¼‰
- list_openalexï¼šå°†äºŒç»´çš„çŸ©é˜µå½¢å¼çš„OpenAlexå­¦ç§‘ï¼Œç»Ÿä¸€åŒ–ä¸ºå•ä¸ªå­¦ç§‘åˆ—è¡¨ï¼Œè®¡ç®—æ–¹å¼å¦‚ä¸‹ï¼š
    1. é¦–å…ˆç”¨æ–°çš„å¤§æ¨¡å‹å¾—åˆ°çš„æ˜ å°„è¡¨é‡æ–°æ˜ å°„ OpenAlex_field_list å’Œ OpenAlex_subfield_list
    2. è®¡ç®—æ¯ä¸ªå­¦ç§‘çš„å¾—åˆ† = ç»Ÿè®¡è¯¥å­¦ç§‘åœ¨æ¯ä¸ªç»´åº¦çš„åˆ†æ•°çš„å¹³å‡å€¼ï¼ˆsum([score for score in 5_dims])ï¼‰,æ²¡åœ¨æŸä¸ªç»´åº¦å‡ºç°å–0
    3. æœ€ç»ˆå¯¹æ¯ä¸ªå­¦ç§‘çš„å¾—åˆ†åšsoftmaxå½’ä¸€åŒ–è¾“å‡ºå‰TopKç»“æœã€‚
- list_refï¼šæŒ‰ç…§å‚è€ƒæ–‡çŒ®çš„OpenAlex_map_subjectsåˆ—è¿›è¡Œç»Ÿä¸€åŒ–è®¡ç®—ï¼Œè®¡ç®—æ–¹å¼ä¸list_openalexç›¸åŒ
"""

import hashlib
import os, ast, json, time
import pandas as pd
import numpy as np
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
from collections import Counter
from typing import List, Tuple, Dict
from tqdm import tqdm
from dotenv import load_dotenv
from utils.vector2discipline import VectorDisciplineScorer, cache_path
from utils.llm_call import call_qwen_rank

load_dotenv()

# ========= ç¯å¢ƒå˜é‡ =========
EMB_MODEL_NAME = os.getenv("EMB_MODEL_NAME", "../../models/bge-m3")
CSV_PATH = os.getenv("CSV_PATH", "../zh_disciplines.csv")
JSON_PATH = os.getenv("JSON_PATH", "../zh_discipline_intro.json")


# ---------- ç ”ç©¶æ–¹å‘ ----------
def add_incites_direction_list(df: pd.DataFrame, mapping_csv: str, direction_col: str = "ç ”ç©¶æ–¹å‘") -> pd.Series:
    """
    ä¸º ç ”ç©¶æ–¹å‘ è¿™ä¸€åˆ—åˆ›å»ºè¾“å…¥ç»´åº¦ list_incites_direction
    :param
        - param df: è¾“å…¥çš„å½“å‰æ•°æ®csvçš„DataFrameæ ¼å¼
        - param mapping_csv: å­˜å‚¨äº†117ä¸ªä¸­å›½ä¸€çº§å­¦ç§‘çš„csvæ–‡ä»¶ï¼Œç”¨äºæ„å»ºå­¦ç§‘ä»£ç åˆ°å­¦ç§‘åç§°çš„æ˜ å°„
        - param direction_col: ç ”ç©¶æ–¹å‘æ‰€åœ¨çš„åˆ—å
    :return
      [('0812 è®¡ç®—æœºç§‘å­¦ä¸æŠ€æœ¯', 0.25), ('0835 è½¯ä»¶å·¥ç¨‹', 0.25)]
    """

    mapping_df = pd.read_csv(mapping_csv, header=None, names=["raw"], dtype=str).fillna("")

    # æ„å»ºä¸€ä¸ªä»å­¦ç§‘ä»£ç åˆ°å­¦ç§‘åç§°çš„æ˜ å°„å­—å…¸
    code2name = {}
    for x in mapping_df["raw"]:
        x = x.strip()
        if len(x) >= 5 and x[:4].isdigit():
            code2name[x[:4]] = x[5:].strip()

    def parse_direction(direction_str: str):
        # è§£æ03openalex_dataä¸­çš„ç ”ç©¶æ–¹å‘å­—æ®µï¼Œè¾“å…¥æ˜¯åˆ†å·åˆ†éš”çš„å­—ç¬¦ä¸²ï¼Œè¾“å‡ºæ˜¯[(å­¦ç§‘ï¼Œåˆ†æ•°)ï¼Œ ...]çš„å½¢å¼
        if not direction_str or not isinstance(direction_str, str):
            return []
        
        # æŒ‰ç…§åˆ†å·åˆ†éš”
        parts = [p.strip() for p in direction_str.split(";") if p.strip()]

        result = []
        for p in parts:
            code = p.split()[0]
            if code.isdigit() and len(code) == 4 and code in code2name:
                result.append(f"{code} {code2name[code]}")

        if not result:
            return []
        score = round(1 / len(result), 4)
        return [(r, score) for r in result]

    return df[direction_col].apply(parse_direction)

# ---------- OpenAlex ----------
def add_openalex_list(df: pd.DataFrame, col: str = "OpenAlex_map_subjects", topk: int = 5) -> pd.Series:
    """
        å¯¹OpenAlexå­—æ®µçš„å­¦ç§‘åˆ—è¡¨è¿›è¡Œåˆ†æ•°è®¡ç®—å’Œæ’åºï¼Œå–topkä¸ªï¼š
        1. é¦–å…ˆç”¨æ–°çš„å¤§æ¨¡å‹å¾—åˆ°çš„æ˜ å°„è¡¨é‡æ–°æ˜ å°„ OpenAlex_field_list å’Œ OpenAlex_subfield_list
        2. è®¡ç®—æ¯ä¸ªå­¦ç§‘çš„å¾—åˆ† = ç»Ÿè®¡è¯¥å­¦ç§‘åœ¨æ¯ä¸ªç»´åº¦çš„åˆ†æ•°çš„å¹³å‡å€¼ï¼ˆsum([score for score in 5_dims])ï¼‰,æ²¡åœ¨æŸä¸ªç»´åº¦å‡ºç°å–0
        3. æœ€ç»ˆå¯¹æ¯ä¸ªå­¦ç§‘çš„å¾—åˆ†åšsoftmaxå½’ä¸€åŒ–è¾“å‡ºå‰TopKç»“æœ

        :parom df: è¾“å…¥çš„å½“å‰æ•°æ®csvçš„DataFrameæ ¼å¼ï¼Œå…¶ä¸­åŒ…å«äº† OpenAlex_field_list å’Œ OpenAlex_subfield_list ä¸¤åˆ—
        :param col: OpenAlex_map_subjectsæ‰€åœ¨çš„åˆ—å
        :param topk: æœ€ç»ˆä¿ç•™çš„Openalexæ˜ å°„åçš„å­¦ç§‘æ•°é‡

        :return å…¨æ–°çš„ OpenAlex_map_subjects åˆ—ï¼Œæ ¼å¼ä»3ç»´åˆ—è¡¨å˜æˆä¸€ç»´åˆ—è¡¨
    """

    # ========== ç¬¬ä¸€æ­¥ï¼šåŠ è½½æ–°çš„æ˜ å°„è¡¨ ==========
    mapping_file = "data/deepseek_map.json"

    openalex_to_cn = {}
    if os.path.exists(mapping_file):
        try:
            with open(mapping_file, "r", encoding="utf-8") as f:
                openalex_to_cn = json.load(f)
            print(f"âœ… å·²åŠ è½½æ–°çš„OpenAlexæ˜ å°„è¡¨ï¼š{mapping_file}")
        except Exception as e:
            print(f"âš ï¸ åŠ è½½æ˜ å°„è¡¨å¤±è´¥: {e}")
    else:
        print(f"âš ï¸ æœªæ‰¾åˆ°æ˜ å°„æ–‡ä»¶: {mapping_file}")

    def map_to_cn_groups(fields: List[str], subfields: List[str]) -> List[List[tuple]]:
        """
        ä¸ºå•ç¯‡è®ºæ–‡çš„æ¯ä¸ª field/subfield ä½¿ç”¨æ–°çš„æ˜ å°„è¡¨å¾—åˆ°æ–°çš„3ç»´åˆ—è¡¨ç»“æ„å­—æ®µ
        """

        groups = []
        for name in (fields or []) + (subfields or []):
            # å¯¹æ¯ä¸ªOpenalexçš„field/subfieldåç§°è¿›è¡Œæ˜ å°„ï¼Œå¾—åˆ°äº†[["å­¦ç§‘", åˆ†æ•°], ...]
            mapped_pairs = openalex_to_cn.get(name, [])

            # å°†åŸæ¥çš„å­—ç¬¦ä¸²çš„å­¦ç§‘å’Œåˆ†æ•°è½¬æ¢ä¸º(str, float)æ ¼å¼
            clean_pairs = []
            for m in mapped_pairs:
                if isinstance(m, (list, tuple)) and len(m) == 2:
                    subj, score = m
                    try:
                        clean_pairs.append((str(subj), float(score)))
                    except Exception:
                        continue
            groups.append(clean_pairs)

        # è¿”å›3ç»´çš„åˆ—è¡¨ç»“æ„ï¼Œä»£è¡¨ä¸€ç¯‡è®ºæ–‡çš„å¤šä¸ªfield/subfieldçš„æ˜ å°„ç»“æœ
        return groups

    # ========== ç¬¬äºŒæ­¥ï¼šé‡æ–°æ˜ å°„OpenAlexæ•°æ® ==========
    def remap_openalex(row):
        """å¯¹ä¸€ç¯‡è®ºæ–‡ï¼Œä½¿ç”¨æ–°çš„æ˜ å°„è¡¨å°†åŸå§‹çš„Openalexå­¦ç§‘ï¼Œæ˜ å°„åˆ°OpenAlex_map_subjectså­—æ®µä¸­"""
        try:
            # è·å–åŸå§‹çš„field_listå’Œsubfield_list
            fields = row.get("OpenAlex_field_list", [])
            subfields = row.get("OpenAlex_subfield_list", [])

            # å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œå°è¯•è§£æä¸ºåˆ—è¡¨
            if isinstance(fields, str):
                fields = ast.literal_eval(fields) if fields.strip() else []
            if isinstance(subfields, str):
                subfields = ast.literal_eval(subfields) if subfields.strip() else []

            # ä½¿ç”¨æ–°æ˜ å°„è¡¨é‡æ–°æ˜ å°„å¾—åˆ°å¯¹åº”ä¸­å›½å­¦ç§‘åŠå…¶åˆ†æ•°
            return map_to_cn_groups(fields, subfields)
        except Exception as e:
            print(f"âš ï¸ é‡æ–°æ˜ å°„OpenAlexå¤±è´¥: {e}")
            return []

    # å¯¹å½“å‰æ•°æ®é›†åº”ç”¨é‡æ–°æ˜ å°„ï¼Œå¾—åˆ°3ç»´åˆ—è¡¨ç»“æ„
    print("ğŸ”„ ä½¿ç”¨æ–°æ˜ å°„è¡¨é‡æ–°æ˜ å°„OpenAlexæ•°æ®...")
    df["OpenAlex_map_subjects_remapped"] = df.apply(remap_openalex, axis=1)

    # ========== ç¬¬ä¸‰æ­¥ï¼šå°†3ç»´åˆ—è¡¨ç»“æ„å˜æˆä¸€ç»´åˆ—è¡¨ç»“æ„ï¼ŒåªåŒ…å«topkä¸ªå­¦ç§‘åŠå…¶åˆ†æ•° ==========
    def aggregate_openalex(subj_list):
        """
        åœ¨ä½¿ç”¨æ–°çš„æ˜ å°„è¡¨é‡æ–°æ˜ å°„åçš„æ•°æ®ä¸Šï¼Œè¿›è¡Œå­¦ç§‘çš„åˆ†æ•°è®¡ç®—å’Œæ’åºï¼Œå–topkä¸ª
        OpenAlex_map_subjectså­—æ®µä»3ç»´åˆ—è¡¨ç»“æ„å˜æˆä¸€ç»´åˆ—è¡¨ç»“æ„

        :param subj_list: é‡æ–°æ˜ å°„åçš„3ç»´åˆ—è¡¨ç»“æ„
        """

        if not subj_list or not isinstance(subj_list, list):
            return []
        try:
            # refs æ˜¯3ç»´åˆ—è¡¨ç»“æ„ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯ä¸€ä¸ªfield/subfieldæ˜ å°„ç»“æœçš„åˆ—è¡¨
            refs = [r for r in subj_list if isinstance(r, list) and r]
            if len(refs) == 0:
                return []

            score_sum = Counter()
            doc_freq = Counter()

            # éå†æ¯ä¸ªOpenalexçš„field/subfieldæ˜ å°„ç»“æœ
            for ref in refs:
                seen_subjs = set()

                # ç»Ÿè®¡ä¸€ä¸ªOpenalexçš„å­¦ç§‘ä¸­ï¼Œæ¯ä¸ªä¸­å›½å­¦ç§‘çš„æ€»åˆ†å’Œå‡ºç°çš„æ¬¡æ•°
                for subj, score in ref:
                    score_sum[subj] += score    # ä¸ºè¿™ä¸ªå­¦ç§‘åŠ åˆ†
                    seen_subjs.add(subj)    # è®°å½•è¿™ä¸ªå­¦ç§‘å‡ºç°è¿‡
                
                # æ›´æ–°æ¯ä¸ªå­¦ç§‘å‡ºç°çš„æ¬¡æ•°
                for s in seen_subjs:
                    doc_freq[s] += 1

            # å¹³å‡å¾—åˆ†ï¼Œæ¯ä¸ªå­¦ç§‘çš„å¾—åˆ† = è¯¥å­¦ç§‘æ€»åˆ† / å‡ºç°æ¬¡æ•°
            final_scores = {subj: score_sum[subj] / doc_freq[subj] for subj in score_sum.keys()}

            # åœ¨å•ä¸ªè®ºæ–‡çš„åˆ†æ•°ä¸Šè¿›è¡Œsoftmaxå½’ä¸€åŒ–
            vals = np.array(list(final_scores.values()), dtype=float)
            e_x = np.exp(vals - np.max(vals))
            probs = e_x / e_x.sum()
            
            # æŒ‰ç…§æ¯ä¸ªå­¦ç§‘çš„å½’ä¸€åŒ–åçš„åˆ†æ•°æ’åºï¼Œå–topkä¸ªï¼Œè¿”å›è¯¥ç»“æœ
            items = sorted(zip(final_scores.keys(), probs), key=lambda x: x[1], reverse=True)[:topk]
            return [(k, round(float(v), 4)) for k, v in items]

        except Exception:
            return []

    # ä½¿ç”¨é‡æ–°æ˜ å°„åçš„æ•°æ®è¿›è¡Œç»Ÿä¸€åŒ–è®¡ç®—
    result = df["OpenAlex_map_subjects_remapped"].apply(aggregate_openalex)

    # æ¸…ç†ä¸´æ—¶åˆ—
    if "OpenAlex_map_subjects_remapped" in df.columns:
        df.drop(columns=["OpenAlex_map_subjects_remapped"], inplace=True)

    return result

# ---------- å‚è€ƒæ–‡çŒ® ----------
def add_ref_list(df: pd.DataFrame, ref_col: str = "Ref_OpenAlex_map_subjects", topk: int = 10) -> pd.Series:
    """
    å¯¹å‚è€ƒæ–‡çŒ®æ‰§è¡Œä¸ OpenAlex å­—æ®µç›¸åŒçš„å¤„ç†é€»è¾‘ï¼Œé¦–å…ˆå¯¹ Ref_OpenAlex_topics å­—æ®µé‡æ–°æ˜ å°„ï¼Œå¾—åˆ°æ–°çš„ Ref_OpenAlex_map_subjects å­—æ®µ
    ç„¶ååœ¨æ–°çš„ Ref_OpenAlex_map_subjects åŸºç¡€ä¸Šè¿›è¡Œç»Ÿä¸€åŒ–è®¡ç®—ï¼Œå¾—åˆ°æœ€ç»ˆçš„å‚è€ƒæ–‡çŒ®å­¦ç§‘åˆ—è¡¨ã€‚

    :param df: è¾“å…¥çš„å½“å‰æ•°æ®csvçš„DataFrameæ ¼å¼ï¼Œå…¶ä¸­åŒ…å«äº† Ref_OpenAlex_topics åˆ—
    :param ref_col: Ref_OpenAlex_map_subjects æ‰€åœ¨çš„åˆ—å
    :param topk: æœ€ç»ˆä¿ç•™çš„å‚è€ƒæ–‡çŒ®æ˜ å°„åçš„å­¦ç§‘æ•°é‡

    :return å…¨æ–°çš„ Ref_OpenAlex_map_subjects åˆ—ï¼Œæ ¼å¼ä»3ç»´åˆ—è¡¨å˜æˆä¸€ç»´åˆ—è¡¨
    """
    # ========== ç¬¬ä¸€æ­¥ï¼šåŠ è½½æ–°çš„æ˜ å°„è¡¨ ==========
    mapping_file = "data/deepseek_map.json"
    openalex_to_cn = {}
    if os.path.exists(mapping_file):
        try:
            with open(mapping_file, "r", encoding="utf-8") as f:
                openalex_to_cn = json.load(f)
            print(f"âœ… å·²åŠ è½½æ–°çš„OpenAlexæ˜ å°„è¡¨ï¼ˆå‚è€ƒæ–‡çŒ®ï¼‰ï¼š{mapping_file}")
        except Exception as e:
            print(f"âš ï¸ åŠ è½½æ˜ å°„è¡¨å¤±è´¥: {e}")
    else:
        print(f"âš ï¸ æœªæ‰¾åˆ°æ˜ å°„æ–‡ä»¶ï¼ˆå‚è€ƒæ–‡çŒ®ï¼‰: {mapping_file}")

        def aggregate_original_ref_subjects(ref_str):
            # ä¸ OpenAlex å­—æ®µç›¸åŒçš„è®¡ç®—é€»è¾‘ï¼Œå°†3ç»´åº¦çš„å‚è€ƒæ–‡çŒ®å­¦ç§‘åˆ—è¡¨ï¼Œè¿›è¡Œç»Ÿä¸€åŒ–è®¡ç®—ï¼Œè½¬æ¢ä¸ºä¸€ç»´åˆ—è¡¨

            if not ref_str or not isinstance(ref_str, str):
                return []
            try:
                # ä»å­—ç¬¦ä¸²è§£æä¸ºåˆ—è¡¨ï¼Œ3ç»´åˆ—è¡¨ç»“æ„
                refs = ast.literal_eval(ref_str)
                refs = [r for r in refs if isinstance(r, list) and r]
                if len(refs) == 0:
                    return []

                score_sum = Counter()
                doc_freq = Counter()

                for ref in refs:
                    seen_subjs = set()
                    for subj, score in ref:
                        score_sum[subj] += score
                        seen_subjs.add(subj)
                    for s in seen_subjs:
                        doc_freq[s] += 1

                # è®¡ç®—æ¯ä¸ªå­¦ç§‘çš„å¹³å‡å¾—åˆ†
                final_scores = {subj: score_sum[subj] / doc_freq[subj] for subj in score_sum.keys()}

                # softmaxå½’ä¸€åŒ–
                vals = np.array(list(final_scores.values()), dtype=float)
                e_x = np.exp(vals - np.max(vals))
                probs = e_x / e_x.sum()

                items = sorted(zip(final_scores.keys(), probs), key=lambda x: x[1], reverse=True)[:topk]
                return [(k, round(float(v), 4)) for k, v in items]

            except Exception:
                return []

        return df[ref_col].apply(aggregate_original_ref_subjects)

    # ========== ç¬¬äºŒæ­¥ï¼šé‡æ–°æ˜ å°„å‚è€ƒæ–‡çŒ®æ•°æ® ==========
    def map_to_cn_groups(fields: List[str], subfields: List[str]) -> List[List[tuple]]:
        """
        ä¸ºå•ç¯‡è®ºæ–‡çš„æ¯ä¸ª field/subfield ä½¿ç”¨æ–°çš„æ˜ å°„è¡¨å¾—åˆ°æ–°çš„3ç»´åˆ—è¡¨ç»“æ„å­—æ®µï¼Œå³ Ref_OpenAlex_map_subjects å­—æ®µ
        """
        groups = []
        for name in (fields or []) + (subfields or []):
            mapped_pairs = openalex_to_cn.get(name, [])
            # ç¡®ä¿æ ¼å¼ä¸º [["å­¦ç§‘", åˆ†æ•°], ...]
            clean_pairs = []
            for m in mapped_pairs:
                if isinstance(m, (list, tuple)) and len(m) == 2:
                    subj, score = m
                    try:
                        clean_pairs.append((str(subj), float(score)))
                    except Exception:
                        continue
            groups.append(clean_pairs)
        return groups

    def remap_ref_topics(row):
        """é‡æ–°æ˜ å°„å‚è€ƒæ–‡çŒ®çš„OpenAlex topicsï¼Œå¾—åˆ°æ–°çš„ Ref_OpenAlex_map_subjects å­—æ®µ"""
        try:
            # è·å–åŸå§‹çš„å‚è€ƒæ–‡çŒ®topics
            ref_topics = row.get("Ref_OpenAlex_topics", [])

            # å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œå°è¯•è§£æä¸ºåˆ—è¡¨
            if isinstance(ref_topics, str):
                ref_topics = ast.literal_eval(ref_topics) if ref_topics.strip() else []

            remapped_refs = []
            for ref in ref_topics:
                if isinstance(ref, list) and len(ref) >= 2:
                    fields = ref[0] if isinstance(ref[0], list) else []
                    subfields = ref[1] if len(ref) > 1 and isinstance(ref[1], list) else []

                    # ä½¿ç”¨æ–°æ˜ å°„è¡¨é‡æ–°æ˜ å°„
                    mapped_groups = map_to_cn_groups(fields, subfields)
                    remapped_refs.append(mapped_groups)

            return remapped_refs
        except Exception as e:
            print(f"âš ï¸ é‡æ–°æ˜ å°„å‚è€ƒæ–‡çŒ®å¤±è´¥: {e}")
            return []

    # æ‰¹é‡åº”ç”¨é‡æ–°æ˜ å°„
    print("ğŸ”„ ä½¿ç”¨æ–°æ˜ å°„è¡¨é‡æ–°æ˜ å°„å‚è€ƒæ–‡çŒ®æ•°æ®...")
    df["Ref_OpenAlex_map_subjects_remapped"] = df.apply(remap_ref_topics, axis=1)

    # ========== ç¬¬ä¸‰æ­¥ï¼šè¿›è¡Œå­¦ç§‘åˆ†æ•°è®¡ç®— ==========
    def aggregate_ref_subjects(ref_list):
        # åœ¨ä½¿ç”¨æ–°çš„æ˜ å°„è¡¨é‡æ–°æ˜ å°„åçš„å‚è€ƒæ–‡çŒ®æ•°æ®ä¸Šï¼Œè¿›è¡Œå­¦ç§‘çš„åˆ†æ•°è®¡ç®—å’Œæ’åºï¼Œå–topkä¸ª

        if not ref_list or not isinstance(ref_list, list):
            return []
        try:
            # å±•å¹³æ‰€æœ‰å‚è€ƒæ–‡çŒ®çš„æ˜ å°„ç»“æœ
            all_refs = []
            for ref in ref_list:
                if isinstance(ref, list):
                    # æ¯ä¸ªrefæ˜¯ä¸€ä¸ªå‚è€ƒæ–‡çŒ®çš„æ˜ å°„ç»“æœåˆ—è¡¨
                    for sublist in ref:
                        if isinstance(sublist, list):
                            all_refs.extend(sublist)

            if len(all_refs) == 0:
                return []

            score_sum = Counter()
            doc_freq = Counter()

            # å¤„ç†æ‰€æœ‰æ˜ å°„åçš„å­¦ç§‘å¯¹
            for subj, score in all_refs:
                score_sum[subj] += score
                doc_freq[subj] += 1

            # è®¡ç®—å¹³å‡å¾—åˆ†
            final_scores = {subj: score_sum[subj] / doc_freq[subj] for subj in score_sum.keys()}

            # softmaxå½’ä¸€åŒ–
            vals = np.array(list(final_scores.values()), dtype=float)
            e_x = np.exp(vals - np.max(vals))
            probs = e_x / e_x.sum()

            items = sorted(zip(final_scores.keys(), probs), key=lambda x: x[1], reverse=True)[:topk]
            return [(k, round(float(v), 4)) for k, v in items]

        except Exception:
            return []

    # æ‰¹é‡ç»Ÿä¸€åŒ–è®¡ç®—
    result = df["Ref_OpenAlex_map_subjects_remapped"].apply(aggregate_ref_subjects)

    # æ¸…ç†ä¸´æ—¶åˆ—
    if "Ref_OpenAlex_map_subjects_remapped" in df.columns:
        df.drop(columns=["Ref_OpenAlex_map_subjects_remapped"], inplace=True)

    return result

# ---------- æ ‡é¢˜+æ‘˜è¦ å…ˆç›¸ä¼¼åº¦å†å¤§æ¨¡å‹åˆ¤æ–­----------
def add_title_abs_scores(df: pd.DataFrame, topn: int = 5, use_gpu: bool = True) -> Tuple[pd.Series, pd.Series, pd.Series, pd.Series, pd.Series]:
    """
    æ ‡é¢˜+æ‘˜è¦å­¦ç§‘è®¡ç®—ï¼ˆåŒå±‚æ¨¡å¼+èåˆï¼Œæ”¯æŒå¹¶å‘+ç¼“å­˜ï¼‰ï¼š
    ------------------------------------------------------------
    1. BGE æ¨¡å‹ç®—è¯­ä¹‰ç›¸ä¼¼åº¦ï¼ˆå…¨117å­¦ç§‘ï¼‰
    2. è°ƒç”¨ Qwen æ¨¡å‹åˆ¤å®šï¼ˆå…¨117å­¦ç§‘ï¼‰
    3. å†æ¬¡è°ƒç”¨ Qwen æ¨¡å‹ï¼ˆä»…è¾“å…¥ BGE å‰5å­¦ç§‘ + ç®€ä»‹ï¼‰
    4. å¹¶å‘æ‰§è¡Œã€ç¼“å­˜å·²è°ƒç”¨ç»“æœ
    5. è¾“å‡ºï¼š
        - list_title_abs_bge       å‘é‡æ¨¡å‹ç»“æœ
        - list_title_abs_qwen      å…¨é‡Qwenç»“æœ
        - list_title_abs_ave       åŸèåˆç»“æœï¼ˆå¹³å‡ï¼‰
        - list_title_abs_merged    é™å®šQwenç»“æœèåˆ
        - list_title_abs           æœ€ç»ˆç»“æœ = list_title_abs_merged
    ------------------------------------------------------------
    """

    # ========== Step 1. BGE æ¨¡å‹ ==========
    scorer = VectorDisciplineScorer(use_gpu=use_gpu)
    code2name, code2intro = scorer.load_disciplines()
    cpath = cache_path(EMB_MODEL_NAME, CSV_PATH, JSON_PATH)
    emb, codes, names, texts = scorer.ensure_cache(cpath, code2name, code2intro)

    text_titleabs = (df["è®ºæ–‡æ ‡é¢˜"] + "ã€‚ " + df["CR_æ‘˜è¦"]).tolist()
    res_titleabs = scorer.score_batch(text_titleabs, codes, names, emb)

    list_bge_all = []
    for r in res_titleabs:
        topn_sorted = sorted(r.items(), key=lambda x: x[1], reverse=True)[:topn]
        list_bge_all.append([(k, float(v)) for k, v in topn_sorted])

    # ========== Step 2. é¢„åŠ è½½å­¦ç§‘åˆ—è¡¨ ==========
    print("ğŸ¤– å‡†å¤‡ Qwen å­¦ç§‘ä¿¡æ¯...")
    with open(CSV_PATH, "r", encoding="utf-8") as f:
        disciplines = [line.strip() for line in f if line.strip() and line[:4].isdigit()]

    # ========== Step 3. ç¼“å­˜è·¯å¾„è®¾ç½® ==========
    CACHE_DIR = "cache/qwen_titleabs"
    os.makedirs(CACHE_DIR, exist_ok=True)

    def cache_key(text: str, mode: str):
        """ç”Ÿæˆç¼“å­˜æ–‡ä»¶è·¯å¾„"""
        h = hashlib.md5(text.encode("utf-8")).hexdigest()[:16]
        return os.path.join(CACHE_DIR, f"{mode}_{h}.json")

    def load_from_cache(text: str, mode: str):
        path = cache_key(text, mode)
        if os.path.exists(path):
            try:
                return json.load(open(path, "r", encoding="utf-8"))
            except:
                return None
        return None

    def save_to_cache(text: str, mode: str, data: dict):
        path = cache_key(text, mode)
        with open(path, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

    # ========== Step 4. å¹¶å‘è°ƒç”¨å‡½æ•° ==========
    def qwen_call(text, bge_res, mode):
        """å•æ¬¡è°ƒç”¨ï¼ˆå¸¦ç¼“å­˜ï¼‰"""
        cache_res = load_from_cache(text, mode)
        if cache_res:
            return cache_res

        try:
            if mode == "all":
                res = call_qwen_rank(
                    text_block=text,
                    disciplines_json=disciplines,
                    disciplines_intro_json=code2intro,
                    topn=topn,
                    mode="title_abs"
                )
            else:
                local_disciplines = [k for k, _ in bge_res]
                local_intro = {k: code2intro.get(k, "") for k in local_disciplines}
                res = call_qwen_rank(
                    text_block=text,
                    disciplines_json=local_disciplines,
                    disciplines_intro_json=local_intro,
                    topn=topn,
                    mode="title_abs"
                )
            save_to_cache(text, mode, res)
            return res if res else bge_res
        except Exception as e:
            print(f"âš ï¸ Qwenè°ƒç”¨å¤±è´¥ ({mode}): {e}")
            return bge_res

    # ========== Step 5. å¹¶å‘æ‰§è¡Œ Qwenï¼Œåˆ†åˆ«å¯¹117ä¸ªå­¦ç§‘åˆ¤æ–­å’Œåœ¨bgeçš„ç»“æœä¸Šåˆ¤æ–­ ==========
    print("âš¡ å¹¶å‘è°ƒç”¨ Qwen æ¨¡å‹åˆ†åˆ«å¯¹117ä¸ªå­¦ç§‘åˆ¤æ–­å’Œå¯¹bgeçš„ç»“æœé‡Œé¢å†åˆ¤æ–­...")
    list_qwen_all = [None] * len(df)
    list_qwen_local_all = [None] * len(df)

    with ThreadPoolExecutor(max_workers=10) as executor:  # å¹¶å‘çº¿ç¨‹æ•°å¯è°ƒ
        futures = {}
        for idx, (text, bge_res) in enumerate(zip(text_titleabs, list_bge_all)):
            futures[executor.submit(qwen_call, text, bge_res, "all")] = ("all", idx)
            futures[executor.submit(qwen_call, text, bge_res, "limited")] = ("limited", idx)

        for fut in tqdm(as_completed(futures), total=len(futures), ncols=90):
            mode, i = futures[fut]
            res = fut.result()
            if mode == "all":
                list_qwen_all[i] = res
            else:
                list_qwen_local_all[i] = res

    # ========== Step 6. èåˆé€»è¾‘ ==========
    def merge_results(bge_list, qwen_list):
        # ç”±bgeçš„ç»“æœå’Œqwençš„ç»“æœè¿›è¡Œèåˆï¼Œå…¶ä¸­qwenç»“æœå¯èƒ½æ˜¯å¯¹117ä¸ªå­¦ç§‘åˆ¤æ–­å¾—åˆ°
        # ä¹Ÿå¯èƒ½æ˜¯åœ¨bgeçš„ç»“æœä¸Šè¿›è¡Œçš„åˆ¤æ–­
        merged = {}
        for k, v in bge_list:
            merged[k] = merged.get(k, 0) + v
        for k, v in qwen_list:
            if k in merged:
                merged[k] = (merged[k] + v) / 2
            else:
                merged[k] = v
        merged_sorted = sorted(merged.items(), key=lambda x: x[1], reverse=True)[:topn]
        return [(k, round(float(v), 4)) for k, v in merged_sorted]

    # BGEç»“æœä¸qwenåœ¨117å­¦ç§‘ä¸Šåˆ¤æ–­ç»“æœè¿›è¡Œåˆ†æ•°èåˆ
    list_ave_all = [merge_results(bge, qwen) for bge, qwen in zip(list_bge_all, list_qwen_all)]
    # BGEç»“æœä¸qwenåœ¨bgeåŸºç¡€ä¸Šåˆ¤æ–­ç»“æœè¿›è¡Œåˆ†æ•°èåˆ
    list_merged_all = [merge_results(bge, qwen_limited) for bge, qwen_limited in zip(list_bge_all, list_qwen_local_all)]

    # æœ€ç»ˆç»“æœ = BGEç»“æœä¸qwenåœ¨bgeåŸºç¡€ä¸Šåˆ¤æ–­ç»“æœè¿›è¡Œåˆ†æ•°èåˆ
    list_final_all = list_merged_all

    print("âœ… å­¦ç§‘è®¡ç®—å®Œæˆï¼å·²å¯ç”¨å¹¶å‘+ç¼“å­˜åŠ é€Ÿã€‚")

    return (
        pd.Series(list_bge_all),
        pd.Series(list_qwen_all),
        pd.Series(list_ave_all),       # åŸå¹³å‡èåˆ
        pd.Series(list_merged_all),    # å±€éƒ¨é™å®šèåˆ
        pd.Series(list_final_all)      # æœ€ç»ˆç»“æœ
    )

# ---------- ä½œè€…æœºæ„ï¼šç›´æ¥è°ƒç”¨ Qwen ----------
def add_author_aff_qwen(df: pd.DataFrame, topk: int = 5, topn_each: int = 2,
                        sleep_time: float = 0.4, max_workers: int = 10) -> pd.Series:
    """
    å¯¹æ¯ç¯‡è®ºæ–‡çš„ä½œè€…æœºæ„å­—æ®µå¹¶å‘è°ƒç”¨ Qwen æ¨¡å‹ï¼š
    ----------------------------------------------------------
    - æ¯ä¸ªæœºæ„åå¹¶å‘è°ƒç”¨ Qwen è¯†åˆ«å­¦ç§‘ï¼ˆæ¯æœºæ„æœ€å¤š topn_each ä¸ªï¼‰
    - è‡ªåŠ¨ç¼“å­˜å·²è¯†åˆ«çš„æœºæ„ç»“æœï¼ˆcache/qwen_author_aff/ï¼‰
    - å¯¹æ‰€æœ‰æœºæ„ç»“æœæ±‚å¹³å‡å softmax å½’ä¸€åŒ–ï¼Œå–å‰ topk è¾“å‡º
    è¾“å‡ºæ ¼å¼ï¼š
      [('0812 è®¡ç®—æœºç§‘å­¦ä¸æŠ€æœ¯', 0.45), ('0835 è½¯ä»¶å·¥ç¨‹', 0.35), ...]
    ----------------------------------------------------------
    """

    # âœ… ä» CSV è¯»å–å­¦ç§‘åˆ—è¡¨
    disciplines = []
    with open(CSV_PATH, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if len(line) >= 5 and line[:4].isdigit():
                disciplines.append(line)

    # âœ… ç¼“å­˜è·¯å¾„è®¾ç½®
    CACHE_DIR = "cache/qwen_author_aff"
    os.makedirs(CACHE_DIR, exist_ok=True)

    def cache_key(text: str):
        """ç”Ÿæˆç¼“å­˜è·¯å¾„"""
        h = hashlib.md5(text.encode("utf-8")).hexdigest()[:16]
        return os.path.join(CACHE_DIR, f"{h}.json")

    def load_cache(text: str):
        path = cache_key(text)
        if os.path.exists(path):
            try:
                return json.load(open(path, "r", encoding="utf-8"))
            except:
                return None
        return None

    def save_cache(text: str, data: list):
        path = cache_key(text)
        with open(path, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

    # âœ… å•æ¬¡è°ƒç”¨å‡½æ•°
    def qwen_call_aff(aff: str):
        """å•ä¸ªæœºæ„è°ƒç”¨ï¼ˆå¸¦ç¼“å­˜ï¼‰"""
        cached = load_cache(aff)
        if cached is not None:
            return aff, cached

        try:
            mapped = call_qwen_rank(
                text_block=aff,
                disciplines_json=disciplines,
                disciplines_intro_json={},
                topn=topn_each,
                mode="author_aff"
            )
            save_cache(aff, mapped or [])
            time.sleep(sleep_time)
            return aff, mapped or []
        except Exception as e:
            print(f"âš ï¸ {aff} è°ƒç”¨å¤±è´¥ï¼š{e}")
            save_cache(aff, [])
            return aff, []

    # âœ… æ”¶é›†æ‰€æœ‰æœºæ„åç§°ï¼ˆå»é‡ï¼‰
    all_affs = set()
    for aff_json_str in df["CR_ä½œè€…å’Œæœºæ„"].fillna("").tolist():
        try:
            aff_list = json.loads(aff_json_str)
            for author in aff_list:
                for aff in author.get("affiliation", []):
                    if isinstance(aff, str) and aff.strip():
                        all_affs.add(aff.strip())
        except Exception:
            continue
    all_affs = list(all_affs)

    print(f"âš¡ å¹¶å‘è°ƒç”¨ Qwen æ¨¡å‹è¿›è¡Œæœºæ„å­¦ç§‘è¯†åˆ«ï¼ˆå…± {len(all_affs)} ä¸ªå”¯ä¸€æœºæ„ï¼‰...")

    # âœ… å¹¶å‘æ‰§è¡Œ
    cache: Dict[str, List[Tuple[str, float]]] = {}
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = {executor.submit(qwen_call_aff, aff): aff for aff in all_affs}
        for fut in tqdm(as_completed(futures), total=len(futures), ncols=90):
            aff, mapped = fut.result()
            cache[aff] = mapped or []

    # âœ… èšåˆæ¯ç¯‡è®ºæ–‡çš„æœºæ„ç»“æœ
    results_all = []
    for _, row in tqdm(df.iterrows(), total=len(df), desc="èšåˆä½œè€…æœºæ„ç»“æœ", ncols=100):
        aff_json_str = row.get("CR_ä½œè€…å’Œæœºæ„", "")
        if not aff_json_str:
            results_all.append([])
            continue

        try:
            aff_list = json.loads(aff_json_str)
        except Exception:
            results_all.append([])
            continue

        score_sum = Counter()
        doc_freq = Counter()

        for author in aff_list:
            for aff in author.get("affiliation", []):
                if not isinstance(aff, str) or not aff.strip():
                    continue
                aff_clean = aff.strip()
                mapped = cache.get(aff_clean, [])
                for subj, score in mapped or []:
                    score_sum[subj] += score
                    doc_freq[subj] += 1

        if not score_sum:
            results_all.append([])
            continue

        # å¹³å‡ + softmax å½’ä¸€åŒ–
        avg_scores = {k: score_sum[k] / doc_freq[k] for k in score_sum.keys()}
        vals = np.array(list(avg_scores.values()), dtype=float)
        e_x = np.exp(vals - np.max(vals))
        probs = e_x / e_x.sum()
        normed = {k: float(v) for k, v in zip(avg_scores.keys(), probs)}

        top_items = sorted(normed.items(), key=lambda x: x[1], reverse=True)[:topk]
        results_all.append([(k, round(v, 4)) for k, v in top_items])

    print("âœ… ä½œè€…æœºæ„å­¦ç§‘è¯†åˆ«å®Œæˆï¼ï¼ˆå·²å¯ç”¨å¹¶å‘+ç¼“å­˜åŠ é€Ÿï¼‰")
    return pd.Series(results_all)

# ---------- ä¸»å‡½æ•° ----------
def make_all_lists(
    input_file: str,
    mapping_csv: str,
    origin_file: str = None,
    output_dir: str = None,
) -> pd.DataFrame:
    """
    æ”¯æŒå¢é‡æ›´æ–°çš„ä¸»å‡½æ•°ï¼š
    1. å¯¹æ¯” origin_fileï¼ˆåŸºçº¿ï¼‰ä¸ input_fileï¼ˆæœ€æ–°ï¼‰â†’ ä»…å¤„ç†æ–°å¢ DOIï¼›
    2. å¯¹æ–°å¢è®ºæ–‡æ‰§è¡Œå„ç±» list ç”Ÿæˆï¼›
    3. è‡ªåŠ¨åˆå¹¶æ—§ç»“æœå¹¶ä¿å­˜è‡³ output_dirï¼›
    """

    ROOT = Path(__file__).resolve().parents[2]
    input_path = (ROOT / input_file).resolve()
    mapping_path = (ROOT / mapping_csv).resolve()
    df_all_new = pd.read_csv(input_path, dtype=str).fillna("")

    # ===== Step 1. å·®é›†è®¡ç®— =====
    if origin_file:
        origin_path = (ROOT / origin_file).resolve()
        if origin_path.is_dir():
            candidate = origin_path / input_path.name
            if candidate.exists():
                origin_path = candidate
                print(f"ğŸ”— åŸºçº¿æ–‡ä»¶åŒ¹é…æˆåŠŸ: {candidate}")
            else:
                print(f"âš ï¸ æœªæ‰¾åˆ°åŸºçº¿æ–‡ä»¶: {candidate}ï¼Œè§†ä¸ºé¦–æ¬¡æ‰§è¡Œã€‚")
                origin_path = None
        else:
            if not origin_path.exists():
                print(f"âš ï¸ æŒ‡å®šåŸºçº¿æ–‡ä»¶ä¸å­˜åœ¨: {origin_path}")
                origin_path = None
    else:
        origin_path = None

    if origin_path and origin_path.exists():
        df_old = pd.read_csv(origin_path, dtype=str).fillna("")
        new_dois = set(df_all_new["DOI"].astype(str))
        old_dois = set(df_old["DOI"].astype(str))
        added = new_dois - old_dois
        print(f"ğŸ” å½“å‰ {len(new_dois)} æ¡ï¼ŒåŸºçº¿ {len(old_dois)} æ¡ â†’ æ–°å¢ {len(added)} æ¡")
        df_new = df_all_new[df_all_new["DOI"].astype(str).isin(added)].reset_index(drop=True)
    else:
        print("âš ï¸ æ— åŸºçº¿æ–‡ä»¶ â†’ å…¨é‡æ‰§è¡Œ")
        df_new = df_all_new

    if df_new.empty:
        print("âœ… æ— æ–°å¢è®ºæ–‡ï¼Œè·³è¿‡ç”Ÿæˆã€‚")
        return df_all_new

    print(f"âš™ï¸ å®é™…éœ€å¤„ç†æ–°å¢è®ºæ–‡æ•°: {len(df_new)}")

    # ===== Step 2. ç”Ÿæˆå„ç±»å­¦ç§‘åˆ—è¡¨ =====
    df = df_new.copy()
    df["list_incites_direction"] = add_incites_direction_list(df, str(mapping_path))
    df["list_ref"] = add_ref_list(df)
    df["list_title_abs_bge"], \
        df["list_title_abs_qwen"], \
        df["list_title_abs_ave"], \
        df["list_title_abs_merged"], \
        df["list_title_abs"] = add_title_abs_scores(df)
    df["list_author_aff_qwen"] = add_author_aff_qwen(df, topk=2)

    if "CR_å‡ºç‰ˆå•†" in df.columns:
        df = df.drop(columns=["CR_å‡ºç‰ˆå•†"])
    if "OpenAlex_map_subjects" in df.columns:
        df["list_openalex"] = add_openalex_list(df, col="OpenAlex_map_subjects")

    # ===== Step 3. æ•°æ®è¿‡æ»¤ =====
    key_cols = [
        "list_incites_direction",
        "list_title_abs",
        "list_author_aff_qwen",
        "list_openalex",
        "list_ref",
    ]

    def _not_empty(v):
        if v is None:
            return False
        if isinstance(v, str):
            v = v.strip()
            if v in ("", "[]", "{}", "null", "None"):
                return False
        if isinstance(v, (list, dict)) and len(v) == 0:
            return False
        return True

    before = len(df)
    df = df[df[key_cols].applymap(_not_empty).all(axis=1)]
    after = len(df)
    print(f"ğŸ§¹ å·²è¿‡æ»¤ç©ºå€¼è®°å½•ï¼š{before - after} æ¡ï¼ˆä¿ç•™ {after} æ¡å®Œæ•´è®°å½•ï¼‰")

    # ===== Step 4. è¾“å‡ºåˆå¹¶ä¿å­˜ =====
    if output_dir:
        out_path = (ROOT / output_dir / input_path.name).resolve()
        os.makedirs(out_path.parent, exist_ok=True)

        if out_path.exists():
            df_old_out = pd.read_csv(out_path, encoding="utf-8-sig")
            before = len(df_old_out)
            df_all = pd.concat([df_old_out, df], ignore_index=True)
            df_all = df_all.drop_duplicates(subset=["DOI"]).reset_index(drop=True)
            print(f"ğŸ§© åˆå¹¶æ—§æ•°æ®ï¼šåŸ {before} æ¡ â†’ åˆå¹¶å {len(df_all)} æ¡")
        else:
            df_all = df
            print("ğŸ†• è¾“å‡ºæ–‡ä»¶ä¸å­˜åœ¨ï¼Œå·²æ–°å»ºã€‚")

        df_all.to_csv(out_path, index=False, encoding="utf-8-sig")
        print(f"âœ… å·²ä¿å­˜å®Œæ•´ç»“æœ â†’ {out_path}")
    else:
        df_all = df
        print("âš ï¸ æœªæŒ‡å®šè¾“å‡ºè·¯å¾„ï¼Œä»…è¿”å› DataFrameã€‚")

    return df_all


# ---------- ç¤ºä¾‹ ----------
if __name__ == "__main__":
    pd.set_option("display.max_colwidth", None)
    make_all_lists(
        "data/03openalex_data/0101 Philosophy.csv",
        "data/zh_disciplines.csv",
        origin_file="data/03origin_openalex_data",
        output_dir="data/04input_data"
    )
