import json
import os

import pandas as pd
from tqdm import tqdm
from utils.subject_calculator import SubjectCalculator
import matplotlib.pyplot as plt
import numpy as np


def align_predicted_with_test(test_data_file, predicted_data_file):
    """æ ¹æ® DOI åŒ¹é… test_data å’Œ predicted_resultï¼Œä½¿ä¸¤è€…è¡Œå¯¹åº”"""
    test_df = pd.read_csv(test_data_file, dtype=str).fillna("")
    pred_df = pd.read_csv(predicted_data_file, dtype=str).fillna("")

    # ç¡®ä¿å­˜åœ¨ DOI åˆ—
    if "DOI" not in test_df.columns or "DOI" not in pred_df.columns:
        raise ValueError("ä¸¤ä¸ªæ–‡ä»¶å¿…é¡»éƒ½åŒ…å« 'DOI' åˆ—")

    # è¿‡æ»¤ predicted_resultï¼Œåªä¿ç•™å‡ºç°åœ¨ test_data ä¸­çš„ DOI
    test_dois = set(test_df["DOI"].unique())
    aligned_pred_df = pred_df[pred_df["DOI"].isin(test_dois)].copy()

    # é‡æ–°æ’åºï¼Œä½¿ predicted æ–‡ä»¶çš„é¡ºåºä¸ test_data ä¸€è‡´
    aligned_pred_df = aligned_pred_df.set_index("DOI").reindex(test_df["DOI"]).reset_index()

    # è¦†ç›–ä¿å­˜ predicted_result.csv
    aligned_pred_df.to_csv(predicted_data_file, index=False, encoding="utf-8-sig")

    print(f"âœ… å·²å¯¹é½ predicted_result.csvï¼Œå…±ä¿ç•™ {len(aligned_pred_df)} æ¡è®°å½•")
    return test_df, aligned_pred_df


def mark_wrong_predictions(test_data, predicted_data, output_dir="data"):
    """
    æ ‡è®°é¢„æµ‹é”™è¯¯çš„è®ºæ–‡ï¼š
    - ä¸»å­¦ç§‘é¢„æµ‹é”™è¯¯ â†’ main_wrong
    - äº¤å‰å­¦ç§‘F1=0 â†’ cross_wrong
    - ä¸¤è€…åŒæ—¶å‡ºç°æ—¶ç”¨ä¸­æ–‡åˆ†å·è¿æ¥
    """
    test_data_with_mark = test_data.copy()
    test_data_with_mark['predict_wrong'] = ''

    columns_to_keep = ['DOI', 'æ¥æº', 'ç ”ç©¶æ–¹å‘', 'primary', 'cross', 'detail']
    existing_columns = [col for col in columns_to_keep if col in predicted_data.columns]

    predicted_data_with_mark = predicted_data[existing_columns].copy()
    predicted_data_with_mark['predict_wrong'] = ''

    wrong_count_main = 0
    wrong_count_cross = 0

    for _, test_row in test_data.iterrows():
        doi = test_row['DOI']
        test_primary = [item.strip()[:4] for item in test_row['primary'].split('ï¼›') if item.strip()]
        test_cross = [item.strip()[:4] for item in test_row['cross'].split('ï¼›') if item.strip()]
        predicted_row = predicted_data[predicted_data['DOI'] == doi]
        if predicted_row.empty:
            continue

        predicted_row = predicted_row.iloc[0]
        predicted_primary = [item.strip()[:4] for item in predicted_row['primary'].split(',') if item.strip()]
        predicted_cross = [item.strip()[:4] for item in predicted_row['cross'].split(',') if item.strip()]

        # --- ä¸»å­¦ç§‘æ­£ç¡®æ€§ ---
        primary_score = 1 if any(item in test_primary for item in predicted_primary) else 0

        # --- äº¤å‰å­¦ç§‘æ–°çš„å‡†ç¡®ç‡ / å¬å›ç‡ / F1 ---
        test_all_labels = set(test_primary + test_cross)
        predicted_cross_set = set(predicted_cross)

        # å¤„ç†æµ‹è¯•äº¤å‰å­¦ç§‘ä¸ºç©ºçš„æƒ…å†µ
        if not test_cross and not predicted_cross_set:
            # å¦‚æœæµ‹è¯•äº¤å‰å­¦ç§‘ä¸ºç©ºï¼Œé¢„æµ‹çš„ä¹Ÿä¸ºç©ºï¼Œåˆ™å‡†ç¡®ç‡ã€å¬å›ç‡ã€F1éƒ½ä¸ºæ»¡åˆ†
            cross_acc = 1.0
            cross_rec = 1.0
            f1_score = 1.0
        elif not test_cross and predicted_cross_set:
            # å¦‚æœæµ‹è¯•äº¤å‰å­¦ç§‘ä¸ºç©ºï¼Œä½†é¢„æµ‹çš„ä¸ä¸ºç©ºï¼Œåˆ™å‡†ç¡®ç‡ä¸º0ï¼Œå¬å›ç‡ä¸º1ï¼ŒF1ä¸º0
            cross_acc = 0.0
            cross_rec = 1.0
            f1_score = 0.0
        elif test_cross and not predicted_cross_set:
            # å¦‚æœæµ‹è¯•äº¤å‰å­¦ç§‘ä¸ä¸ºç©ºï¼Œä½†é¢„æµ‹çš„ä¸ºç©ºï¼Œåˆ™å‡†ç¡®ç‡ä¸º1ï¼Œå¬å›ç‡ä¸º0ï¼ŒF1ä¸º0
            cross_acc = 1.0
            cross_rec = 0.0
            f1_score = 0.0
        else:
            # æ­£å¸¸æƒ…å†µï¼šä¸¤è€…éƒ½ä¸ä¸ºç©º
            correct_cross = len(predicted_cross_set & test_all_labels)
            cross_acc = correct_cross / len(predicted_cross_set) if predicted_cross_set else 0
            cross_rec = correct_cross / len(test_all_labels) if test_all_labels else 0
            f1_score = (2 * cross_acc * cross_rec) / (cross_acc + cross_rec) if cross_acc + cross_rec else 0

        # --- é”™è¯¯æ ‡è®° ---
        mark_labels = []
        if primary_score == 0:
            mark_labels.append("main_wrong")
            wrong_count_main += 1
        if f1_score == 0:
            mark_labels.append("cross_wrong")
            wrong_count_cross += 1

        mark_str = 'ï¼›'.join(mark_labels) if mark_labels else ''
        if mark_str:
            test_data_with_mark.loc[test_data_with_mark['DOI'] == doi, 'predict_wrong'] = mark_str
            predicted_data_with_mark.loc[predicted_data_with_mark['DOI'] == doi, 'predict_wrong'] = mark_str

    # --- ä¿å­˜ç»“æœ ---
    test_output_file = os.path.join(output_dir, "test_data_marked.csv")
    predicted_output_file = os.path.join(output_dir, "predicted_result_marked.csv")

    test_data_with_mark.to_csv(test_output_file, index=False, encoding="utf-8-sig")
    predicted_data_with_mark.to_csv(predicted_output_file, index=False, encoding="utf-8-sig")

    print(f"ğŸ’¾ å·²ç”Ÿæˆæ ‡è®°æ–‡ä»¶: test_data_marked.csv ä¸ predicted_result_marked.csv")
    print(f"ğŸ“Š ä¸»å­¦ç§‘é¢„æµ‹é”™è¯¯(main_wrong): {wrong_count_main}")
    print(f"ğŸ“Š äº¤å‰å­¦ç§‘F1=0 (cross_wrong): {wrong_count_cross}")


def refrank_with_llm_and_stub_result(input_dir="data/04input_data", output_dir="data/05output_data", file_path=None):
    """é˜¶æ®µå››ï¼šç›´æ¥è®¡ç®—å­¦ç§‘ç»“æœï¼ˆå¯æŒ‡å®šå•ä¸ªæ–‡ä»¶ï¼‰"""
    os.makedirs(output_dir, exist_ok=True)
    files = [os.path.basename(file_path)] if file_path else sorted(
        [f for f in os.listdir(input_dir) if f.endswith(".csv")]
    )
    if file_path:
        input_dir = os.path.dirname(file_path)

    calc = SubjectCalculator(debug=True, strategy="lr", topk_cross=3)
    out_csv = os.path.join(output_dir, "predicted_result.csv")
    results_json = []

    for fname in files:
        in_csv = os.path.join(input_dir, fname)
        print(f"â¡ï¸ æ­£åœ¨å¤„ç†æ–‡ä»¶: {fname}")

        df = pd.read_csv(in_csv, dtype=str).fillna("")
        for _, row in tqdm(df.iterrows(), total=len(df), desc="  âš™ï¸ è®¡ç®—", ncols=90, leave=False):
            try:
                res = calc.calc(row)
                results_json.append(json.dumps(res, ensure_ascii=False))
            except Exception:
                results_json.append("{}")

        df["result"] = results_json
        df["primary"] = df["result"].apply(lambda x: json.loads(x).get("primary"))
        df["cross"] = df["result"].apply(lambda x: ",".join(json.loads(x).get("cross", [])))
        df["detail"] = df["result"].apply(lambda x: json.dumps(json.loads(x).get("detail", {}), ensure_ascii=False))
        df.to_csv(out_csv, index=False, encoding="utf-8-sig")

    return df


def calculate_accuracy(test_data_file, predicted_data_file, output_dir="data"):
    """è®¡ç®—å‡†ç¡®ç‡ã€å¬å›ç‡å’ŒF1åˆ†æ•°ï¼Œå¹¶æ‰“å°æ¯ç¯‡è®ºæ–‡çš„è¯¦ç»†è®¡ç®—ç»“æœ"""
    test_data = pd.read_csv(test_data_file, dtype=str).fillna("")
    predicted_data = pd.read_csv(predicted_data_file, dtype=str).fillna("")

    primary_correct_count = 0
    total_primary_count = len(test_data)

    primary_correct_list, cross_accuracy_list, cross_recall_list, f1_score_list = [], [], [], []

    print("\nğŸ“Š ========== æ¯ç¯‡è®ºæ–‡çš„è®¡ç®—ç»“æœ ==========")
    for i, test_row in enumerate(test_data.itertuples(), 1):
        test_primary = [item.strip()[:4] for item in getattr(test_row, 'primary').split('ï¼›') if item.strip()]
        test_cross = [item.strip()[:4] for item in getattr(test_row, 'cross').split('ï¼›') if item.strip()]
        predicted_row = predicted_data[predicted_data['DOI'] == getattr(test_row, 'DOI')]
        if predicted_row.empty:
            continue
        predicted_row = predicted_row.iloc[0]
        predicted_primary = [item.strip()[:4] for item in predicted_row['primary'].split(',') if item.strip()]
        predicted_cross = [item.strip()[:4] for item in predicted_row['cross'].split(',') if item.strip()]

        # === ä¸»å­¦ç§‘å‡†ç¡®æ€§ ===
        primary_score = 1 if any(item in test_primary for item in predicted_primary) else 0
        primary_correct_count += primary_score

        # === ä¿®æ”¹åçš„äº¤å‰å­¦ç§‘å‡†ç¡®ç‡ / å¬å›ç‡è®¡ç®— ===
        test_all_labels = set(test_primary + test_cross)
        predicted_cross_set = set(predicted_cross)

        # å¤„ç†æµ‹è¯•äº¤å‰å­¦ç§‘ä¸ºç©ºçš„æƒ…å†µ
        if not test_cross and not predicted_cross_set:
            # å¦‚æœæµ‹è¯•äº¤å‰å­¦ç§‘ä¸ºç©ºï¼Œé¢„æµ‹çš„ä¹Ÿä¸ºç©ºï¼Œåˆ™å‡†ç¡®ç‡ã€å¬å›ç‡ã€F1éƒ½ä¸ºæ»¡åˆ†
            cross_accuracy = 1.0
            cross_recall = 1.0
            f1_score = 1.0
        elif not test_cross and predicted_cross_set:
            # å¦‚æœæµ‹è¯•äº¤å‰å­¦ç§‘ä¸ºç©ºï¼Œä½†é¢„æµ‹çš„ä¸ä¸ºç©ºï¼Œåˆ™å‡†ç¡®ç‡ä¸º0ï¼Œå¬å›ç‡ä¸º1ï¼ŒF1ä¸º0
            cross_accuracy = 0.0
            cross_recall = 1.0
            f1_score = 0.0
        elif test_cross and not predicted_cross_set:
            # å¦‚æœæµ‹è¯•äº¤å‰å­¦ç§‘ä¸ä¸ºç©ºï¼Œä½†é¢„æµ‹çš„ä¸ºç©ºï¼Œåˆ™å‡†ç¡®ç‡ä¸º1ï¼Œå¬å›ç‡ä¸º0ï¼ŒF1ä¸º0
            cross_accuracy = 1.0
            cross_recall = 0.0
            f1_score = 0.0
        else:
            # æ­£å¸¸æƒ…å†µï¼šä¸¤è€…éƒ½ä¸ä¸ºç©º
            correct_cross = len(predicted_cross_set & test_all_labels)
            cross_accuracy = correct_cross / len(predicted_cross_set) if predicted_cross_set else 0
            cross_recall = correct_cross / len(test_all_labels) if test_all_labels else 0
            f1_score = (2 * cross_accuracy * cross_recall) / (
                        cross_accuracy + cross_recall) if cross_accuracy + cross_recall else 0

        primary_correct_list.append(primary_score)
        cross_accuracy_list.append(cross_accuracy)
        cross_recall_list.append(cross_recall)
        f1_score_list.append(f1_score)

        # === æ‰“å°æ¯ç¯‡è®ºæ–‡çš„è¯¦ç»†ç»“æœ ===
        print(f"\nğŸ“„ [{i}] DOI: {getattr(test_row, 'DOI', '(æ— )')}")
        print(f"  ğŸ·ï¸ æµ‹è¯•ä¸»å­¦ç§‘: {test_primary}")
        print(f"  ğŸ§  é¢„æµ‹ä¸»å­¦ç§‘: {predicted_primary} {'âœ…' if primary_score else 'âŒ'}")
        print(f"  ğŸ”— æµ‹è¯•äº¤å‰å­¦ç§‘: {test_cross}")
        print(f"  ğŸ” é¢„æµ‹äº¤å‰å­¦ç§‘: {predicted_cross}")
        print(f"     â†³ å‡†ç¡®ç‡={cross_accuracy:.3f}  å¬å›ç‡={cross_recall:.3f}  F1={f1_score:.3f}")

    # === æ±‡æ€»ç»Ÿè®¡ - ä½¿ç”¨å¹³å‡å€¼ ===
    primary_accuracy = primary_correct_count / total_primary_count if total_primary_count else 0
    cross_accuracy = np.mean(cross_accuracy_list) if cross_accuracy_list else 0
    cross_recall = np.mean(cross_recall_list) if cross_recall_list else 0
    cross_f1 = np.mean(f1_score_list) if f1_score_list else 0

    print("\nğŸ“ˆ ======= æ€»ä½“æŒ‡æ ‡æ±‡æ€» =======")
    print(f"ä¸»å­¦ç§‘å‡†ç¡®ç‡: {primary_accuracy:.4f}")
    print(f"äº¤å‰å­¦ç§‘å‡†ç¡®ç‡: {cross_accuracy:.4f}")
    print(f"äº¤å‰å­¦ç§‘å¬å›ç‡: {cross_recall:.4f}")
    print(f"äº¤å‰å­¦ç§‘F1åˆ†æ•°: {cross_f1:.4f}")

    # ä¿å­˜å¸¦æ ‡è®°æ–‡ä»¶
    mark_wrong_predictions(test_data, predicted_data, output_dir)
    plot_accuracy(primary_correct_list, cross_accuracy_list, cross_recall_list, f1_score_list)


def plot_accuracy(primary_correct_list, cross_accuracy_list, cross_recall_list, f1_score_list):
    plt.figure(figsize=(12, 8))
    plt.subplot(2, 2, 1)
    n, bins, patches = plt.hist(primary_correct_list, bins=[0, 1, 2], edgecolor="black", color="skyblue")
    plt.title("Primary Discipline Match")
    plt.xticks([0.5, 1.5], ["Not Matched", "Matched"])
    plt.xlabel("Match Status")
    plt.ylabel("Count")
    for patch in patches:
        plt.text(patch.get_x() + patch.get_width() / 2, patch.get_height() + 0.1, f'{int(patch.get_height())}',
                 ha='center')

    for i, (title, data, color) in enumerate(
            [("Cross Discipline Accuracy", cross_accuracy_list, "salmon"),
             ("Cross Discipline Recall", cross_recall_list, "lightgreen"),
             ("Cross Discipline F1 Score", f1_score_list, "orange")], start=2):
        plt.subplot(2, 2, i)
        bins = np.arange(0, 1.1, 0.1)
        n, bins, patches = plt.hist(data, bins=bins, edgecolor="black", color=color)
        plt.title(title)
        plt.xticks(np.arange(0, 1.1, 0.1))
        plt.xlabel(title.split()[-1])
        plt.ylabel("Count")
        for patch in patches:
            plt.text(patch.get_x() + patch.get_width() / 2, patch.get_height() + 0.1, f'{int(patch.get_height())}',
                     ha='center')

    plt.tight_layout()
    plt.show()


def main():
    input_data_dir = '../../data/04input_data'
    output_data_dir = '../../data'
    test_data_file = '../../data/test_data.csv'
    predicted_data_file = os.path.join(output_data_dir, "predicted_result.csv")

    print("å¼€å§‹æ‰§è¡Œé¢„æµ‹ç»“æœç”Ÿæˆ...")
    refrank_with_llm_and_stub_result(input_dir=input_data_dir, output_dir=output_data_dir,
                                     file_path='../../data/04input_data/1205 Library and Information Science & Archive Management.csv')

    print("å¼€å§‹å¯¹é½ predicted_result.csv ä¸ test_data.csv ...")
    test_data, aligned_pred_data = align_predicted_with_test(test_data_file, predicted_data_file)

    print("å¼€å§‹è®¡ç®—å‡†ç¡®ç‡...")
    calculate_accuracy(test_data_file, predicted_data_file, output_dir=output_data_dir)


if __name__ == "__main__":
    main()