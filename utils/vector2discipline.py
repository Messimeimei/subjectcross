# -*- coding: utf-8 -*-
"""
VectorDisciplineScorer
- å¯¹117ä¸ªä¸€çº§å­¦ç§‘çš„â€œå­¦ç§‘ä»‹ç»chunkâ€å‘é‡åŒ–ï¼ˆç¼“å­˜ï¼‰
- æä¾›å•æ¡ä¸æ‰¹é‡çš„æ–‡æœ¬â†’å­¦ç§‘ç›¸ä¼¼åº¦è®¡ç®—
- FAISS æ”¯æŒæ‰¹é‡æ£€ç´¢
- é‡è¦ï¼šå§‹ç»ˆè¿”å› 117 å­¦ç§‘å…¨é‡åˆ†æ•°ï¼ˆä¸åšä»»ä½• TopN æ©ç ï¼‰
"""

import os, json, re
from typing import List, Tuple, Dict
from collections import defaultdict
import numpy as np
import pandas as pd
import torch
from sentence_transformers import SentenceTransformer
from dotenv import load_dotenv

load_dotenv()

# ========= ç¯å¢ƒå˜é‡ =========
# ç»Ÿä¸€æ›¿æ¢
EMB_MODEL_NAME = os.getenv("EMB_MODEL_NAME", "models/bge-m3")
CSV_PATH = os.getenv("CSV_PATH", "../data/zh_disciplines.csv")
JSON_PATH = os.getenv("JSON_PATH", "../data/zh_discipline_intro.json")
CACHE_DIR = os.getenv("CACHE_DIR", "../models/bge-m3/.cache_embeddings")
os.makedirs(CACHE_DIR, exist_ok=True)

BATCH_SIZE = int(os.getenv("BATCH_SIZE", 128))
USE_FP16 = os.getenv("USE_FP16", "true").lower() == "true"
TARGET_CH_LEN = int(os.getenv("TARGET_CH_LEN", 220))
TOPK_EACH_DISC = int(os.getenv("TOPK_EACH_DISC", 3))   # èšåˆæŸå­¦ç§‘æ—¶å–è¯¥å­¦ç§‘å‘½ä¸­çš„å‰Kä¸ªchunkçš„å¹³å‡
MAX_RETRIEVE = int(os.getenv("MAX_RETRIEVE", 2000))    # FAISS æ£€ç´¢è¿”å›çš„å€™é€‰ä¸Šé™


# ========= å·¥å…·å‡½æ•° =========
def get_device():
    try:
        import torch
        return "cuda" if torch.cuda.is_available() else "cpu"
    except Exception:
        return "cpu"

def _hash_of(text: str) -> str:
    import hashlib
    return hashlib.md5(text.encode("utf-8")).hexdigest()

def cache_path(model_name: str, csv_path: str, json_path: str) -> str:
    sig = json.dumps({
        "model": model_name,
        "csv": os.path.abspath(csv_path),
        "json": os.path.abspath(json_path),
        "params": {"TARGET_CH_LEN": TARGET_CH_LEN}
    }, ensure_ascii=False)
    return os.path.join(CACHE_DIR, f"chunks_{_hash_of(sig)}.npz")

def split_chinese_intro(text: str, target_len: int = TARGET_CH_LEN):
    if not text or not isinstance(text, str):
        return []
    s = re.sub(r"\r\n?", "\n", text).strip()
    seps = "ã€‚ï¼ï¼Ÿï¼›;\n"
    buf, pieces = "", []
    for ch in s:
        buf += ch
        if ch in seps and len(buf) >= target_len:
            pieces.append(buf.strip()); buf = ""
    if buf.strip():
        pieces.append(buf.strip())
    merged, cur = [], ""
    for p in pieces:
        if len(cur) + len(p) < target_len // 2:
            cur += p
        else:
            if cur: merged.append(cur); cur = ""
            merged.append(p)
    if cur: merged.append(cur)
    merged = [m for m in merged if len(m) >= 20]
    if not merged and s:
        merged = [s[:max(200, target_len)]]
    return merged


# ========= ä¸»ç±» =========
class VectorDisciplineScorer:
    def __init__(self, model_path=EMB_MODEL_NAME, use_gpu=True):
        device = get_device()
        self.device = "cuda" if use_gpu and device == "cuda" else "cpu"
        self.model = SentenceTransformer(model_path, device=self.device)
        if self.device == "cuda" and USE_FP16:
            try:
                self.model = self.model.half()
            except Exception:
                pass
        # âœ… åˆå§‹åŒ–åæ¸…ç©ºç¼“å­˜ + æ˜¾å­˜ä¿¡æ¯
        if self.device == "cuda":
            torch.cuda.empty_cache()
            mem = torch.cuda.get_device_properties(0).total_memory / 1024 ** 3
            print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆ")

    # ---------- å­¦ç§‘åº“ ----------
    def load_disciplines(self, csv_path=CSV_PATH, json_path=JSON_PATH):
        df = pd.read_csv(csv_path, sep=r"\s+", header=None, names=["code", "name"], engine="python")
        df["code"] = df["code"].astype(str).str.zfill(4)
        code2name = dict(df.values)
        with open(json_path, "r", encoding="utf-8") as f:
            raw = json.load(f)
        code2intro = {code: v.get("intro", "") for code, v in raw.items()}
        return code2name, code2intro

    def build_chunks(self, code2name, code2intro):
        rows = []
        for code, name in code2name.items():
            intro = code2intro.get(code, "")
            chunks = split_chinese_intro(intro, TARGET_CH_LEN)
            for c in chunks:
                rows.append((code, name, c))
        return rows

    def embed_texts(self, texts: List[str], batch_size=BATCH_SIZE) -> np.ndarray:
        """å®‰å…¨æ‰¹é‡å‘é‡åŒ–ï¼šè‡ªåŠ¨é™æ‰¹ + OOMå›é€€CPU"""
        import gc, time

        results = []
        cur_bs = min(batch_size, 64)  # åˆå§‹æœ€å¤§ 64
        for start in range(0, len(texts), cur_bs):
            sub_texts = texts[start:start + cur_bs]
            for _ in range(3):  # æœ€å¤šé‡è¯•3æ¬¡
                try:
                    vecs = self.model.encode(
                        sub_texts,
                        batch_size=cur_bs,
                        show_progress_bar=False,
                        normalize_embeddings=True,
                        convert_to_numpy=True,
                        device=self.device
                    )
                    results.append(vecs)
                    break  # æˆåŠŸåˆ™è·³å‡ºé‡è¯•
                except torch.cuda.OutOfMemoryError:
                    print(f"âš ï¸ GPUæ˜¾å­˜ä¸è¶³ï¼ˆbatch={cur_bs}ï¼‰ï¼Œè‡ªåŠ¨å‡åŠé‡è¯•...")
                    torch.cuda.empty_cache()
                    gc.collect()
                    cur_bs = max(1, cur_bs // 2)
                    time.sleep(1)
                except Exception as e:
                    print(f"âŒ å‘é‡åŒ–å‡ºé”™: {e}")
                    torch.cuda.empty_cache()
                    gc.collect()
                    time.sleep(1)
                    # é™çº§åˆ° CPU
                    print("âš ï¸ åˆ‡æ¢åˆ° CPU æ¨¡å¼ç»§ç»­ç¼–ç ...")
                    vecs = self.model.encode(
                        sub_texts,
                        batch_size=max(1, cur_bs // 2),
                        show_progress_bar=False,
                        normalize_embeddings=True,
                        convert_to_numpy=True,
                        device="cpu"
                    )
                    results.append(vecs)
                    break
            torch.cuda.empty_cache()
            gc.collect()

        out = np.vstack(results).astype("float32")
        return out

    def ensure_cache(self, cache_file: str, code2name, code2intro, force_rebuild=False):
        if os.path.exists(cache_file) and not force_rebuild:
            data = np.load(cache_file, allow_pickle=True)
            return data["emb"], data["codes"].tolist(), data["names"].tolist(), data["texts"].tolist()
        chunks = self.build_chunks(code2name, code2intro)
        texts = [t[2] for t in chunks]
        emb = self.embed_texts(texts)
        codes = [t[0] for t in chunks]
        names = [t[1] for t in chunks]
        np.savez_compressed(cache_file, emb=emb,
                            codes=np.array(codes, dtype=object),
                            names=np.array(names, dtype=object),
                            texts=np.array(texts, dtype=object))
        return emb, codes, names, texts

    # ---------- FAISS æ£€ç´¢ï¼ˆæ”¯æŒæ‰¹é‡ queryï¼‰ ----------
    def _faiss_index(self, dim):
        import faiss
        try:
            if faiss.get_num_gpus() > 0 and self.device == "cuda":
                res = faiss.StandardGpuResources()
                return faiss.GpuIndexFlatIP(res, dim)
            else:
                return faiss.IndexFlatIP(dim)
        except Exception:
            return faiss.IndexFlatIP(dim)

    def faiss_search_batch(self, query_vecs: np.ndarray, emb: np.ndarray, topk: int):
        """
        æ‰¹é‡æ£€ç´¢ï¼šquery_vecs shape=(N, dim)
        è¿”å›ï¼šD shape=(N, topk), I shape=(N, topk)
        """
        import faiss
        dim = emb.shape[1]
        index = self._faiss_index(dim)
        index.add(emb)
        if query_vecs.ndim == 1:
            query_vecs = query_vecs.reshape(1, -1)
        topk = min(topk, emb.shape[0])
        D, I = index.search(query_vecs, topk)
        return D, I

    # ---------- æ‰“åˆ†æ ¸å¿ƒï¼ˆå§‹ç»ˆè¿”å› 117 å­¦ç§‘å…¨é‡ï¼‰ ----------
    def _aggregate_scores_by_code(
        self,
        I_row: np.ndarray,
        D_row: np.ndarray,
        codes: List[str],
        names: List[str],
    ) -> Dict[str, float]:
        """
        æŠŠä¸€æ¡ query çš„æ£€ç´¢ç»“æœèšåˆåˆ°â€œå­¦ç§‘ä»£ç  å­¦ç§‘åâ€ç»´åº¦ã€‚
        - å…ˆæŠŠå±äºåŒä¸€å­¦ç§‘ä»£ç çš„å¤šä¸ª chunk ç›¸ä¼¼åº¦åš TopK å‡å€¼ï¼ˆTOPK_EACH_DISCï¼‰ã€‚
        - è¿”å› 117 å­¦ç§‘å…¨é‡åˆ†æ•°ï¼ˆæœªå‘½ä¸­=0.0ï¼Œä¸åšä»»ä½• TopN æ©ç ï¼‰ã€‚
        """
        # ä½™å¼¦ï¼ˆå†…ç§¯ï¼‰[-1,1] â†’ [0,1]
        dense_norm = (D_row + 1.0) / 2.0
        by_disc = defaultdict(list)
        for pos, idx in enumerate(I_row):
            by_disc[codes[idx]].append(float(dense_norm[pos]))

        code2name = {c: n for c, n in zip(codes, names)}
        scores_full: Dict[str, float] = {}
        for code in codes:
            vals = by_disc.get(code, [])
            if vals:
                vals_sorted = sorted(vals, reverse=True)[:max(1, TOPK_EACH_DISC)]
                score = float(np.mean(vals_sorted))
            else:
                score = 0.0
            scores_full[f"{code} {code2name[code]}"] = score
        return scores_full

    # ---------- å•æ¡æ¥å£ ----------
    def _score_single_text(self, text: str, codes, names, emb) -> Dict[str, float]:
        q = self.embed_texts([text])[0]
        D, I = self.faiss_search_batch(q, emb, min(MAX_RETRIEVE, emb.shape[0]))
        return self._aggregate_scores_by_code(I[0], D[0], codes, names)

    def score_from_affiliations(self, authors_affils: List[Tuple[str, str]], codes, names, texts, emb) -> Dict[str, float]:
        affils = []
        for a in authors_affils:
            if isinstance(a, (list, tuple)) and len(a) == 2:
                affils.append(a[1])
            elif isinstance(a, str):
                affils.append(a)
        text = " ".join(affils) if affils else ""
        return self._score_single_text(text, codes, names, emb)

    def score_from_journal(self, journal_name: str, codes, names, texts, emb) -> Dict[str, float]:
        return self._score_single_text(journal_name or "", codes, names, emb)

    def score_from_title_abstract(self, title: str, abstract: str, codes, names, texts, emb) -> Dict[str, float]:
        return self._score_single_text((title or "") + "\n" + (abstract or ""), codes, names, emb)

    # ---------- æ‰¹é‡æ¥å£ ----------
    def score_batch(self, texts: List[str], codes, names, emb) -> List[Dict[str, float]]:
        """
        æ‰¹é‡è¾“å…¥æ–‡æœ¬ â†’ è¿”å›æ¯æ¡æ–‡æœ¬çš„ 117 å­¦ç§‘å…¨é‡åˆ†æ•°å­—å…¸
        """
        if not texts:
            return []
        q_vecs = self.embed_texts(texts)  # GPU æ‰¹é‡å‘é‡åŒ–
        D, I = self.faiss_search_batch(q_vecs, emb, min(MAX_RETRIEVE, emb.shape[0]))  # FAISS æ‰¹é‡æ£€ç´¢
        out = []
        for r in range(q_vecs.shape[0]):
            out.append(self._aggregate_scores_by_code(I[r], D[r], codes, names))
        return out


# ========= ä½¿ç”¨ç¤ºä¾‹ï¼ˆå¯é€‰ï¼‰=========
if __name__ == "__main__":
    scorer = VectorDisciplineScorer(EMB_MODEL_NAME, use_gpu=True)
    code2name, code2intro = scorer.load_disciplines(CSV_PATH, JSON_PATH)
    cpath = cache_path(EMB_MODEL_NAME, CSV_PATH, JSON_PATH)
    emb, codes, names, texts = scorer.ensure_cache(cpath, code2name, code2intro)

    # å•æ¡ï¼ˆè¿”å› 117 å…¨é‡ï¼‰
    r = scorer.score_from_journal("Science Education", codes, names, texts, emb)
    print("=== å•æ¡ç»“æœ ===")
    for k, v in r.items():
        print(f"{k}: {v:.4f}")
    print("ğŸ”¥ Top3:", sorted(r.items(), key=lambda x: x[1], reverse=True)[:3])

    # æ‰¹é‡ï¼ˆè¿”å› 117 å…¨é‡ï¼‰
    batch_texts = [
        "Deep learning for protein structure prediction",
        "Machine learning approaches in clinical epidemiology",
    ]
    res = scorer.score_batch(batch_texts, codes, names, emb)

    print("\n=== æ‰¹é‡ç»“æœ ===")
    for i, d in enumerate(res, 1):
        print(f"\næ ·æœ¬ {i} å…¨éƒ¨å­¦ç§‘åˆ†æ•°ï¼š")
        for k, v in d.items():
            print(f"{k}: {v:.4f}")
        print(f"ğŸ”¥ Top3: {sorted(d.items(), key=lambda x: x[1], reverse=True)[:3]}")
